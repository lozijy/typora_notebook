# 前言

## 为什么学这门课程

本人致力于制作一个操作系统内核Lonux，在此之前已经自学了C语言，汇编语言，了解制作操作系统的大概流程，看过部分intel手册，但无奈对操作系统的具体理论仍然知之甚少，所以我期望通过久负盛名的MIT 6.S081实验来作为我制作操作系统的理论依据，本博客仅供学习使用，参考了github user @duguoshen的博客并进行了相应的拓展，如涉嫌侵权会自行删除.

## 总览

关于6.s081的学习从24,Oct,2022开始，于xx,xx,xxx全部结束

课程分为8章，对应12个lab

课程:

1：操作系统接口:

2：操作系统架构

3：页表:

4：陷阱指令和系统调用:

5：中断和设备驱动:

6：锁

7：调度

8：文件系统

lab:

1：通过预先写好的系统调用写一些user程序.

2：自行添加拓展一些新的系统调用

3：对页表进行修改

4：使用陷阱实现系统调用



# 第负二章 RISCV手册导读

基于书籍:RISCV中文手册

## RISC-V Convention

### RISCV

ISA:指令集架构

RISCV可以理解为指令集，每个处理器都有关联的ISA或指令，就是一套指令集，每条指令都有相关的二进制编码或操作码

大多数现代计算机运行在X86上，这是一个不同的ISA，IntelCPU，AMDCpu实现了X86

RISC-V指的是精简指令集，X86是复杂指令集，Intel手册里有三本书参考ISA和参考数据，新的指令依然以每个月3个的速度增加，当然还有其他精简指令集，如ARM，MAC正在向ARM指令集转型。

RISCV指令集有2个文档，包括了特权指令和非特权指令,RISCV是少数的开源操作系统之一，是伯克利大学的实验室项目，这意味着任何厂商都可以为这个指令集设计电路板，Intel这么大的原因是他更注重向后的兼容性

### Register Convention

#### 整数寄存器和浮点值寄存器

函数在整数寄存器**a0**和**a1**以及浮点寄存器**fa0**和**fa1**中返回值。只有当浮点值是原始值（传入时**fa0**和**fa1**作为参数寄存器，原始值是指该参数不改变而直接返回）或作为仅有一两个浮点值组成的结构体的成员时，才会从浮点寄存器中返回。长度恰好为两个指针字长的其他返回值将在**a0**和**a1**中返回。较大的返回值完全在内存中传递；调用方分配此内存区域，并将指针作为隐式的第一个参数传递给被调用方。

在标准的RISC-V调用约定中，栈向下增长，栈指针始终保持16字节对齐。

当在堆栈上传递两倍于指针字大小的基本参数时，它们是自然对齐的。当它们在整数寄存器中传递时，它们驻留在对齐的偶数号-奇数号寄存器对中，偶数寄存器保存最低有效位。例如，在RV32中，函数`void foo(int, long long)`的第一个参数在**a0**中传递，第二个参数在**a2**和**a3**中传递。**a1**中不传递任何内容。

大于指针字大小两倍的参数通过引用传递。

除了自变量和返回值寄存器之外，还有在调用中不稳定的七个整数寄存器**t0**-**t6**和十二个浮点寄存器**ft0**-**ft11**作为临时寄存器，如果之后使用，调用者必须保存它们。十二个整数寄存器**s0**-**s11**和十二个浮点寄存器**fs0**-**fs11**在调用中受保护，如果使用，被调用者必须保存它们。表18.2显示了调用约定中每个整数和浮点寄存器的作用。

![image-20221128165849032](https://raw.githubusercontent.com/lozijy/image/main/image-20221128165849032.png)

![image-20221128165905423](https://raw.githubusercontent.com/lozijy/image/main/image-20221128165905423.png)

基本思想是RISCV普通指令是64位，但有一个压缩版本，指令是16位，

调用者保存寄存器在函数调用期间不会保留而被调用这寄存器会保留，调用者保存寄存器可以呗函数重写，

比如函数a调用函数B,函数a使用的任何寄存器都是调用者使用的，调用函数b可以在其被调用时覆盖返回地址中的值，你可以看到返回地址都是调用者保存的，这很重要，因为每个函数都需要返回地址,b能覆盖返回地址中的值是很重要的

#### 寄存器总览

32个通用寄存器,还有一些其他寄存器，如PC,MODE,SATP,STVEC,SEPC,SSCRATCH

#### stvec寄存器

stvec寄存器是处理中断指令的地址，sepc寄存器是用于在trap期间保存程序计数器PC，还有个叫sscratch的寄存器

第一步保存32个通用寄存器的状态方便恢复用户进程的状态，我们希望内核为中断提供服务，用户进程恢复后不会意识到发生了什么，这32个通用寄存器不能被内核干扰

我们今天讨论1.为了安全和隔离，不希望用户进程影响到内核程序的执行，这意味着trap中涉及的各种硬件和内核机制不能指望任何来自用户空间的东西，不能用到通用寄存器的东西，因为可能包含恶意值，只是保存，不会查看这些通用寄存器，其次我们希望这对用户是透明的，这是为了让编写用户代码方便。

#### MODE寄存器

寄存器MODE,模式标志控制的是什么?模式可以是用户模式，页可以是管理者模式，我们从用户模式变成管理者模式后可以在内核中执行代码，但获得的特权相当有限，

1.可以读写控制寄存器，SATP页表指针 stvec指针控制trap进入内核的位置，这个寄存器在trap期间保存程序计数器。 sscratch寄存器加某些值

2.可以使用不带PTE_U(带了的表示可以被用户读写)的PTE

但是管理者模式也有很多不能做的，比如读写物理地址

这节课我们会花大量时间在gdb上，查看内核中的trap条目的返回值

## 栈

the stack 

stack frame 栈帧，每当我们调用一个函数函数，函数就会创造一个栈帧，并通过移动栈指针(sp)来使用它，

对于栈，我们从高地址开始，然后向下拓展到低地址

函数栈帧包括寄存器，局部变量，如果参数寄存器用完了，额外的参数就会出现在栈上，栈帧不一定是相同大小。返回地址总在第一个，两个重要的寄存器，SP，在栈的底部，也就是栈的位置，fp指向当前帧的底部，这很重要，意味着返回地址和前一fp将始终位于当前帧指针的固定位置，我们之所以存储前一个帧指针，是为了可以向后跳转，一旦这个函数调用，我们可以把它移动到fp，然后fp从指向上一个栈帧到指向这一个栈帧。 

![image-20221128170238244](https://raw.githubusercontent.com/lozijy/image/main/image-20221128170238244.png)



## 中断

有三种事件会导致中央处理器搁置普通指令的执行，并强制将控制权转移到处理该事件的特殊代码上。一种情况是系统调用，当用户程序执行`ecall`指令要求内核为其做些什么时；另一种情况是异常：（用户或内核）指令做了一些非法的事情，例如除以零或使用无效的虚拟地址；第三种情况是设备中断，一个设备，例如当磁盘硬件完成读或写请求时，向系统表明它需要被关注。

## RISCV指令集架构

## RISCV架构下的汇编语言

# 第负一章 操作系统前置知识

这是我自己总结的操作系统基本理论，基于计算机操作系统第四版(西安电子科技大学出版社),操作系统概念(机械工业出版社)~~不学点操作系统理论去做6.s081真的好难.~~

## 操作系统的目标

1.方便

操作系统通过编译命令将高级语言书写的程序翻译成机器语言，或者用户可以通过操作系统提供的接口管理计算机

为程序员提供大量系统调用

2.有效

OS通过各种算法，比如说页面置换算法，进程调度算法，作业调度算法等等加快程序的运行，缩短程序的运行周期，提高系统的吞吐量

3.可扩充

这些年广泛采用来微内核的设计，由于采用的是C/S模式，与硬件高度紧密的部分都设计在微内核中，而关于进程管理，内存管理，文件管理，磁盘管理功能的实现都存在于用户态的服务器中，因此在不同的硬件上跑的时候需改动的地方较小，具有良好的可扩充型.

值得注意的是，linux,windows,xv6等最火的操作系统都是采用的是宏内核，微内核一般应用在嵌入式系统中，因为微内核进行更多次的核心态用户态切换导致性能下降严重.

4.开放

系统能遵循世界标准规范，使彼此能相互兼容.比如常见的POSIX标准

## 操作系统的作用

1.作为用户和计算机硬件系统之间的接口

2.作为计算机系统资源的管理者

3.实现了对计算机资源的抽象

## 操作系统的特性

### 并发:

并行性:两个或多个事件在同一时刻发生

并发性:两个或多个事件在同一时间间隔内发生

并发是在宏观上同时发生，在微观上是交替执行的

如果计算机有多个处理器，则这些并发执行的程序便可分配到多个处理器上实现并行执行



引入了进程的概念，进程是指在系统中能独立运行并作为资源分配的基本单位，它是由一组机器指令，数据和堆栈等组成，是一个能独立运行的活动实体

### 共享:

共享指系统资源被并发执行的多个进程共同使用，资源中的资源远小于多道程序需求的总和，会形成它们对共享资源的争夺，因此操作系统要对资源共享进行妥善处理

资源共享方式:

1.互斥共享

针对打印机等资源，一段时间内只能有一个进程对资源进行访问，这种资源叫临界资源系统中大多数物理设备，如栈，变量，表格等

虚拟:

异步:

## 操作系统的主要功能

### 处理机:

进程控制

进程同步

进程通信

进程调度

### 存储器:

内存分配

内存保护

地址映射

内存扩充

### 设备管理:

缓冲管理

设备分配

设备处理

### 文件管理:

文件存储空间管理

目录管理

文件的读写管理和保护



# 第零章 环境配置和相关工具的使用

## 环境搭建

#### 下载相应工具集

```shell
sudo apt-get install git build-essential gdb-multiarch qemu-system-misc gcc-riscv64-linux-gnu binutils-riscv64-linux-gnu 
```

c/c++编译工具集，riscv64架构的gnu工具集,qemu工具集，git,适用于riscv架构的gdb调试工具

#### 下载MIT改动后的Xv6源代码

下载Xv6的git仓库,注意不要下载Xv6的官方源码，而是要下载课程改动过的源码

```shell
git clone git://g.csail.mit.edu/xv6-labs-2021

```

#### 测试

进入文件夹输入以下内容:

```shell
git checkout util
make qemu
```

#### 配置ubuntu的ssh服务器

输入sudo su进入root用户模式，然后输入passwd为root用户设置一个密码。

输入ls /etc/ssh命令查看是否有sshd_config这个文件，如果没有则输入apt-get install openssh-server下载。

将sshd_config中的PermitRootLogin的#号去掉，并将值改为yes。

输入service ssh restart重启ssh服务。（如果没设置开机自动启动，那么每次开机都要重启ssh服务）

```shell
删除,重装ssh
$ sudo apt-get remove --purge openssh-server
$ sudo apt-get update
$ sudo apt-get install openssh-server
```

#### 在clion上连接虚拟机

![image-20221103132445217](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132445217.png)

![image-20221103132546715](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132546715.png)

![image-20221103132607147](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132607147.png)

![image-20221103132734841](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132734841.png)

![image-20221103132744094](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132744094.png)

把debugger改成multiarch因为原始gdb无法识别riscv的指令集

![image-20221103132906540](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103132906540.png)

#### 配置部署服务器

点击上方**工具**，选择**部署**，点击**配置**，填好映射路径。

![image-20221103133841592](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103133841592.png)

然后依次点**工具**、**部署**、**浏览远程主机**，就可以管理虚拟机的文件了，在这里可以很方便的直接管理虚拟机文件。

删除项目内初始的文件（如果有的话），右键项目文件夹，点击部署，然后下载虚拟机里的文件。



配置Makefile项目
打开设置，将构建目标设置为qemu-gdb。

*如果设置里显示未检测到Makefile项目就删掉项目文件夹里的.idea*文件夹，然后重新打开项目。**之前的设置会被初始化，需要重新设置一下。

设置完了之后打开Makefile文件，点右上角按钮重新加载项目。

![image-20221103135937314](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221103135937314.png)

此时索引就建立完毕了，可以自动补全和ctrl+B来快速跳转了。

#### 添加调试配置

在右上角打开**运行/调试配置**，添加一个**远程调试**的配置。

![image-20220329124021990](https://img-blog.csdnimg.cn/img_convert/3267d2e938cceb2f1496baae815d85e9.png)

课程内的gdb服务器默认使用*25000*端口。

执行`make`后会对内核程序和和每个用户程序各生成一个符号文件，内核的符号文件就叫*kernel*，用户程序的符号文件为_{程序名}，例如ls的符号文件为*_ls*。



设置.gdbinit
在*/root文件夹下创建一个名为.gdbinit*的文件

向里面填入如下内容

```shell
set confirm off
set architecture riscv:rv64
file kernel/kernel
add-symbol-file user/_ls
set disassemble-next-line auto
set riscv use-compressed-breakpoints yes
```

这里是调试ls，如果要调试自己写的程序，比如说myproc，就添加一行add-symbol-file user/_myproc。

#### 启动调试服务器

在虚拟机的Xv6文件夹下输入`make qemu-gdb`来启动gdb服务器，如遇到错误可以尝试`make clean`来清除之前make生成的文件，shell输出以下内容即为开启成功。

```shell
*** Now run 'gdb' in another window.
qemu-system-riscv64 -machine virt -bios none -kernel kernel/kernel -m 128M -smp 3 -nographic -drive file=fs.img,if=none,format=raw,id=x0 -device virtio-blk-device,drive=x0,bus=virtio-mmio-bus.0 -S -gdb tcp::25000
```

如果显示25000端口已被占用或者文件fs.img被占用，应该是上一次运行没有正确退出，请自行百度查找如何根据占用端口或者文件获取pid，然后kill掉，这里建议使用lsof。

apt-get install lsof安装lsof。
lsof fs.img和lsof -i:25000查询进程。

#### 开始调试

点击CLion右上角调试按钮开始调试ls程序，下方会显示连接成功，然后回到shell会发现可以输入命令了，此时我输入`ls grep`，成功触发断点。

![image-20220329131621170](https://img-blog.csdnimg.cn/img_convert/c591e5329c4bd34062c47fb0f279a6a0.png)

#### xv6和clion相关

Xv6没有实现ps程序，要列出当前进程可以在Xv6的终端里中输入ctrl+p，关闭Xv6先按ctrl+a再按x。
调试Xv6时发生读取空指针，缓冲区溢出等错误时CLion不会报错，但是会导致调试异常，当系统中断，跳转到spinlock.c等文件内时需要格外注意。
在CLion内可以用ctrl+alt+h来查看一个函数被哪些函数调用了。

##　



## GNU Debugger

配置RISCV-GNU-toolchain

RISC-V架构下利用QEMU进行GDB调优

使用GNUDebugger

- 带有GDB：运行`make qemu[-nox]-gdb`，然后在第二个Shell中启动GDB（`iscv64-linux-gnu-gdb`）

- 如果以单核方式启动，则使用`make CPUS=1 qemu-gdb`

- 不带有GDB：当不需要GDB时使用`make qemu[-nox]`命令

- `step`一次运行一行代码。当有函数调用时，它将步进到被调用的对象函数。

- `next`也是一次运行一行代码。但当有函数调用时，它不会进入该函数。

- `stepi`和`nexti`对于汇编指令是单步调试。

- `continue`运行代码，直到遇到断点或使用`<Ctrl-c>`中断它

- `finish`运行代码，直到当前函数返回

- `advance <location>`运行代码，直到指令指针到达指定位置

- `break <location>`在指定的位置设置断点。 位置可以是内存地址(`*0x7c00`)或名称(`monbacktrace`，`monitor.c:71`)

- 如需修改断点请使用`delete`，`disable`，`enable`

- `x`以您指定格式（`x/x`表示十六进制，`x/i`表示汇编，等等）打印内存的原始内容。

- `print`计算一个C表达式并将结果以合适的类型打印。它通常比`x`更有用

- 使用`p *((struct elfhdr *) 0x10000)`的输出比`x/13x 0x10000`的输出好得多

- `info registers`打印每个寄存器的值

- `info frame`打印当前栈帧

- `list <location>`在指定位置打印函数的源代码

- `backtrace`或许对于你的lab1中的工作很有用处

- GDB有一个文本用户界面，在curses用户界面中显示有用的信息，如代码列表、反汇编和寄存器内容

  - `layout <name>`切换到给定的用户界面

  例如`layout split`，效果如下

  `layout asm`：查看汇编 `layout reg`：查看寄存器 `info reg`：查看寄存器 `b *0x1234`：在指定地址设定断点

  `Ctrl+a c`：进入控制模式 `info mem`：打印页表x`
  
  
  
  

## GCC命令

![image-20221201182643963](https://raw.githubusercontent.com/lozijy/image/main/image-20221201182643963.png)

## makefile

makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。makefile带来的好处就是——“自动化编译”，一旦写好，只需要一个make命令，整个工程完全自动编译，极大的提高了软件开发的效率。make是一个命令工具，是一个解释makefile中指令的命令工具，一般来说，大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。可见，makefile都成为了一种在工程方面的编译方法。

首先要把源文件编译成***\*中间代码文件\****，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做***\*编译（compile）\****。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。  



**链接时**，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“***\*库文件”（Library File)\****，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。

#### 规则

我们的规则是：
      1.如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。

​      2.如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。

​      3.如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。

#### 语法

 **target... : prerequisites ...**

​     **command**

​     **...**

​     **...
​     -------------------------------------------------------------------------------**

​    **target**也就是一个目标文件，可以是**Object File**，也可以是执行文件。还可以是一个标签（Label），对于标签这种特性，在后续的“伪目标”章节中会有叙述。

​    **prerequisites**就是，要生成那个target所需要的文件或是目标。

​    **command**也就是make需要执行的命令。（任意的Shell命令）

 【注】：在看别人写的Makefile文件时，你可能会碰到以下三个变量：$@，$^，$<代表的意义分别是： 

​          他们三个是十分重要的三个变量，所代表的含义分别是：

​          $@--目标文件，$^--所有的依赖文件，$<--第一个依赖文件。



```makefile
clean:
	@rm -rf $(BUILD_DIR) || exit 1
run: $(BUILD_DIR)/$(OS_ISO)
	@qemu-system-i386 $(QEMU_OPTIONS)
	@sleep 1
	@telnet 127.0.0.1 $(QEMU_MON_PORT)
```





```makefile
   edit : main.o kbd.o command.o display.o \

          insert.o search.o files.o utils.o

           cc -o edit main.o kbd.o command.o display.o \

                      insert.o search.o files.o utils.o

 

   main.o : main.c defs.h

           cc -c main.c

   kbd.o : kbd.c defs.h command.h

           cc -c kbd.c

   command.o : command.c defs.h command.h

           cc -c command.c

   display.o : display.c defs.h buffer.h

           cc -c display.c

   insert.o : insert.c defs.h buffer.h

           cc -c insert.c

   search.o : search.c defs.h buffer.h

           cc -c search.c

   files.o : files.c defs.h buffer.h command.h

           cc -c files.c

   utils.o : utils.c defs.h

           cc -c utils.c

   clean :

           rm edit main.o kbd.o command.o display.o \

              insert.o search.o files.o utils.o
```

##### 变量

设置变量objcts

```makefile
 objects = main.o kbd.o command.o display.o \

             insert.o search.o files.o utils.o
 
```

于是，我们就可以很方便地在我们的makefile中以***\*“$(objects)”\****的方式来使用这个变量了，于是我们的改良版makefile就变成下面这个样子：

```makefile
   objects = main.o kbd.o command.o display.o \
             insert.osearch.o files.o utils.o 
   edit : $(objects)
           cc -o edit $(objects)
   main.o : main.c defs.h
           cc -c main.c
   kbd.o : kbd.c defs.h command.h
           cc -c kbd.c
   command.o : command.c defs.h command.h
           cc -c command.c
   display.o : display.c defs.h buffer.h
           cc -c display.c
   insert.o : insert.c defs.h buffer.h
           cc -c insert.c
   search.o : search.c defs.h buffer.h
           cc -c search.c
   files.o : files.c defs.h buffer.h command.h
           cc -c files.c
   utils.o : utils.c defs.h
           cc -c utils.c
   clean :
           rm edit $(objects)
```

于是如果有新的 .o 文件加入，我们只需简单地修改一下 objects 变量就可以了。



##### make自动推导

GNU的make可以自动推导文件以及文件依赖关系后的命令,make会自动识别并自己推导命令

```makefile
  objects = main.o kbd.o command.o display.o \
             insert.o search.o files.o utils.o
 
   edit : $(objects)
           cc -o edit $(objects)
 
   main.o : defs.h
   kbd.o : defs.h command.h
   command.o : defs.h command.h
   display.o : defs.h buffer.h
   insert.o : defs.h buffer.h
   search.o : defs.h buffer.h
   files.o : defs.h buffer.h command.h
   utils.o : defs.h
 
   .PHONY : clean
   clean :
           rm edit $(objects)
```

“.PHONY”表示，clean是个伪目标文件。



##### 另类风格的makefile

```makefile
   objects = main.o kbd.o command.o display.o \
             insert.o search.o files.o utils.o
 
   edit : $(objects)
           cc -o edit $(objects)
 
   $(objects) : defs.h
   kbd.o command.o files.o : command.h
   display.o insert.o search.o files.o : buffer.h
 
   .PHONY : clean
   clean :
           rm edit $(objects)
```



##### include

在Makefile使用include关键字可以把别的Makefile包含进来，这很像C语言的#include，被包含的文件会原模原样的放在当前文件的包含位置。

  include foo.make *.mk $(bar)

等价于：

  include foo.make a.mk b.mk c.mk e.mk f.mk

make命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找：

```sh
1.如果make执行时，有“-I”或“--include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。
2.如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。
```

```makefile
include config/make-locations
include config/make-os
include config/make-cc
include config/make-debug-tool
```

 如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果你想让make不理那些无法读取的文件，而继续执行，你可以在include前加一个减号“-”。如：

-include<filename>



##### 环境变量

建议不看

##### make的工作方式

```sh
1.        读入所有的Makefile。

2.        读入被include的其它Makefile。

3.        初始化文件中的变量。

4.        推导隐晦规则，并分析所有规则。

5.        为所有的目标文件创建依赖关系链。

6.        根据依赖关系，决定哪些目标要重新生成。

7.        执行生成命令。
```

#### Makefile书写规则

##### 通配符

##### 文件搜寻

vpath

##### 伪目标

.PHONY clean

##### 多目标

多个目标依赖同一个命令可以会出现问题，我们有自动化变量把其合并起来

``` makefile
  bigoutput littleoutput : text.g

      generate text.g -$(subst output,,$@) > $@
```

  上述规则等价于：

```makefile
   bigoutput : text.g

           generate text.g -big > bigoutput

   littleoutput : text.g

           generate text.g -little > littleoutput
```



其中，-$(subst output,,$@)中的“$”表示执行一个Makefile的函数，函数名为subst，后面的为参数。关于函数，将在后面讲述。这里的这个函数是截取字符串的意思，“$@”表示目标的集合，就像一个数组，“$@”依次取出目标，并执于命令。

##### 静态模式

静态模式更容易定义多目标的规则，语法

```makefile
<targets...>: <target-pattern>: <prereq-patterns ...>

　　　<commands>
```

 targets定义一系列目标文件

target-pattern指明了targets的模式

prereq-patterns是目标的依赖模式，它对target-pattern形成的模式再进行一次依赖目标的定义



  

 ```makefile
 objects = foo.o bar.o
 
  
 
   all: $(objects)
 
 
   $(objects): %.o: %.c
 
       $(CC) -c $(CFLAGS) $< -o $@
 ```



上面的例子中，指明了我们的目标从$object中获取，“%.o”表明要所有以“.o”结尾的目标，也就是“foo.o bar.o”，也就是变量$object集合的模式，而依赖模式“%.c”则取模式“%.o”的“%”，也就是“foobar”，并为其加下“.c”的后缀，于是，我们的依赖目标就是“foo.cbar.c”。而命令中的“$<”和“$@”则是自动化变量，“$<”表示所有的依赖目标集（也就是“foo.c bar.c”），“$@”表示目标集（也褪恰癴oo.o bar.o”）。于是，上面的规则展开后等价于下面的规则：



另一个例子:

```makefile
 files = foo.elc bar.o lose.o

 

   $(filter %.o,$(files)): %.o: %.c

           $(CC) -c $(CFLAGS) $< -o $@

   $(filter %.elc,$(files)): %.elc: %.el

           emacs -f batch-byte-compile $<
```

$(filter%.o,$(files))表示调用Makefile的filter函数，过滤“$filter”集，只要其中模式为“%.o”的内容。这个例字展示了Makefile中更大的弹性。

##### 自动生成依赖

在Makefile中，我们的依赖关系可能会需要包含一系列的头文件，比如，如果我们的main.c中有一句“#include "defs.h"”，那么我们的依赖关系应该是：

  main.o : main.c defs.h



#### 变量

#### 函数

#### make的运行



# 第一章 Operating system interfaces

## xv6简介

xv6模仿了Unix的内部设计

xv6采用传统的内核形式--宏内核

## 系统接口

xv6提供了丰富的系统接口

这也是最常见的Linux/POSIX的系统调用

![image-20221126164515155](https://raw.githubusercontent.com/lozijy/image/main/image-20221126164515155.png)

![image-20221126164544969](https://raw.githubusercontent.com/lozijy/image/main/image-20221126164544969.png)

## 进程

每个正在运行的程序，称为进程，都有包含指令、数据和堆栈的内存。



### 进程与系统调用,核心态与用户态



一个进程需要调用一个内核服务时，它会调用一个系统调用，这是操作系统接口中的一个调用。系统调用进入内核；内核执行服务并返回。因此，一个进程在用户空间和内核空间之间交替执行。

![image-20221125235415871](https://raw.githubusercontent.com/lozijy/image/main/image-20221125235415871.png)

内核使用CPU提供的硬件保护机制来确保每个在用户空间执行的进程只能访问它自己的内存。内核程序的执行拥有操控硬件的权限，它需要实现这些保护；而用户程序执行时没有这些特权。当用户程序调用系统调用时，硬件会提升权限级别，并开始执行内核中预先安排好的函数。

### fork

创建一个子进程，考虑编译后的二进制文件，子进程会完全复制这些二进制文件，在内存中，他们拥有相同的命令，数据和栈，而且他们都认为自己是从虚拟内存0开始执行的。他们都会抢占cpu资源，所以他们执行的操作通常是交叉进行的(比如说我们fork一个进程，然后打印一串字符"hello",那么就会显示如"hhellloelo"这样如同乱码的字符)。

返回:如果是父进程，就会返回0,如果是子进程就会返回pid,所以调用fork的程序我们一般要用if判断是子进程还是父进程来执行不同的操作

```c
int pid=fork();
if(pid!=0){
    pid=wait((int *)0)
    printf("parent!");
}else{
    printf("child!");
    exit(0)
}
```

### exec

exec系统调用使用从文件系统中存储的文件所加载的新内存映像替换调用进程的内存。

exec系统调用有两个参数,文件名和参数args,args是一个字符串数组,第一项通常是要执行的文件名，往往跳过，最后一项为0表示字符串数组已结束

例如:

```c
char* argv[3];
argv[0] = "echo";
argv[1] = "hello";
argv[2] = 0;
exec("/bin/echo", argv);
printf("exec error\n");
```





## I/O和文件描述符

文件描述符是个小整数，表示进程可以读取或写入的由内核管理的对象，可以打开一个文件，目录，设备或创建一个管道，或复制一个已经存在的描述符来获取一个文件描述符。

文件描述符将文件，管道，设备之间的差异抽象出来，使他们看起来都像字节流

按照惯例，进程从文件描述符0读取（标准输入），将输出写入文件描述符1（标准输出），并将错误消息写入文件描述符2（标准错误）。正如我们将看到的，shell利用这个约定来实现I/O重定向和管道。shell确保它始终有三个打开的文件描述符（***user/sh.c\***:151），这是控制台的默认文件描述符。

![image-20221126003400460](https://raw.githubusercontent.com/lozijy/image/main/image-20221126003400460.png)

### write和read

#### read:

![image-20221126003928101](https://raw.githubusercontent.com/lozijy/image/main/image-20221126003928101.png)

第一个参数就是文件描述符，读的时候会把读取的内容放进buf里面,如果文件结束，返回0.

引用文件描述符的时候会伴随一个偏移量，读取内容时从当前偏移量开始，把内容读进buf时会伴随偏移量的前进.

#### write:

系统调用`write(fd，buf，n)`将buf中的n字节写入文件描述符，并返回写入的字节数。只有发生错误时才会写入小于n字节的数据。与读一样，`write`在当前文件偏移量处写入数据，然后将该偏移量向前推进写入的字节数：每个`write`从上一个偏移量停止的地方开始写入。



#### 小程序

```c
char buf[512];
int n;
for (;;) {
    n = read(0, buf, sizeof buf);
    if (n == 0)
        break;
    if (n < 0) {
        fprintf(2, "read error\n");
        exit(1);
    }
    if (write(1, buf, n) != n) {
        fprintf(2, "write error\n");
        exit(1);
    }
}
```

### close

close函数关闭一个文件描述符，使其可以被未来调用的open,pipe,dup系统调用重用，新分配的文件描述符总是选取当前进程中编号最小的未使用的文件描述符.

### open ,dup,pipe

#### open

![image-20221126005705848](https://raw.githubusercontent.com/lozijy/image/main/image-20221126005705848.png)

打开一个文件并为这个文件分配一个文件描述符

这是open的flags的定义:

```shell
(kernel/fcntl.h)
#define O_RDONLY  0x000		//Open for reading only
#define O_WRONLY  0x001    //Open for writing only
#define O_RDWR    0x002    // Open for reading and writing
#define O_CREATE  0x200    // Create the file with the mode permissions if file does not exist
#define O_TRUNC   0x400    //If the file exists and is a regular file, and the file is successfully opened O_RDWR or O_WRONLY, its length is truncated to 0 and the mode and owner are unchanged
```

我们可以利用close和open系统调用实现一个简单的重定向

```c
char* argv[2];
argv[0] = "cat";
argv[1] = 0;
if (fork() == 0) {
    close(0);
    open("input.txt", O_RDONLY);
    exec("cat", argv);
}
```

这样我们关闭了文件描述符0，后面open给input.txt分配了文件描述符0,在后面执行cat系统调用的时候的输入就不是标准输入而是input.txt文件作为输入.

相当于Linux下的shell代码:

```sh
cat <input.txt
```

需要注意的几点:

1.fork和exec是常见的一对系统调用组合.

fork创建一个子进程，考虑编译后的二进制文件，子进程会完全复制这些二进制文件.

exec系统调用使用从文件系统中存储的文件所加载的新内存映像替换调用进程的内存.

使用fork+exec的组合会使fork替换掉的是新生成的子进程而不会对父进程产生影响。

但从上面的例子中可以看出为什么不会有forkexec这样的一个系统调用，因为在fork后我们可以通过close,open等系统调用后再exec实现重定向,考虑一个假想的forkexec系统调用的话，他虽然也能实现执行其他系统调用的功能，但在重定向方面是很笨拙的

2.虽然fork复制了文件描述符表，但基础文件偏移量在子进程和父进程之间是共享的，考虑如下程序

```c
if (fork() == 0) {
    write(1, "hello ", 6);
    exit(0);
} else {
    wait(0);
    write(1, "world\n", 6);
}
```

会在标准输出中打印hello world而非

world

hello

因为基础文件偏移量在子父进程之间共享，这样有利于从shell命令序列产生顺序输出，比如(echo hello;echo world)>output.txt

#### dup

dup系统调用复制一个现有的文件描述符，返回一个新的文件描述符，和fork产生的文件描述符一样，二者的现有的文件描述符和新的文件描述符的基础文件偏移量也是共享的，考虑以下代码:

```c
fd = dup(1);
write(1, "hello ", 6);
write(fd, "world\n", 6);
```

也是会输出hello world 

#### pipe

![image-20221127202752005](https://raw.githubusercontent.com/lozijy/image/main/image-20221127202752005.png)

p[0]是读端,p[1]写端

pipe是作为一对文件描述符公开给进程的小型内核缓冲区，

我们常常有类似以下的代码实现重定向redirect

```c
int p[2];
char *argv[2];
argv[0] = "wc";
argv[1] = 0;
pipe(p);
if (fork() == 0) {
    close(0);
    dup(p[0]);
    close(p[0]);
    close(p[1]);
    exec("/bin/wc", argv);
} else {
    close(p[0]);
    write(p[1], "hello world\n", 12);
    close(p[1]);
}

```

子进程关闭读段，往写端里写入helloword后再关闭写端

父进程先关闭了标准输入，而使用dup把文件描述符0赋给管道的读端,然后再关闭读端和写端,因为然后exec的时候就会直接从管道的读端中获取参数,而不是从标准输入中读.

下面是xv6源码中(user/sh.c\:100)针对runcmd针对cmd结构体的TYPE是PIPE情况的写法,子进程创建一个pipe将管道的左端和右端连接起来，然后对管道的左端调用fork和runcmd，对管道的右端调用fork和cmd,并等待两者都完成。管道的右边可能是一个命令，该命令本身包含一个管道，该管道本身fork为快歌新的子进程，因此，shell可以创建一个进程树，这个树的叶子是命令，内部节点是等待左右两个进程完成的进程。

![image-20221127203152912](https://raw.githubusercontent.com/lozijy/image/main/image-20221127203152912.png)

原则上，可以让内部节点在管道的左端运行，但是正确地这样做会使实现复杂化。考虑进行以下修改：将***sh.c\***更改为不对`p->left`进行`fork`，并在内部进程中运行`runcmd(p->left)`。然后，例如，`echo hi | wc`将不会产生输出，因为当`echo hi`在`runcmd`中退出时，内部进程将退出，而不会调用`fork`来运行管道的右端。这个不正确的行为可以通过不调用内部进程的`runcmd`中的`exit`来修复，但是这个修复使代码复杂化：现在`runcmd`需要知道它是否是一个内部进程。同样的，当没有对`(p->right)`执行`fork`时也会更加复杂。例如，只需进行上述的修改，`sleep 10 | echo hi`将立即打印“hi”，而不是在10秒后，因为`echo`将立即运行并退出，而不是等待`sleep`完成。因为***sh.c\***的目标是尽可能的简单，所以它不会试图避免创建内部进程。

管道看起来并不比临时文件更强大：下面的管道命令行

```bash
echo hello world | wc
```

可以不通过管道实现，如下

```bash
echo hello world > /tmp/xyz; wc < /tmp/xyz
```

在这种情况下，管道相比临时文件至少有四个优势

- 首先，管道会自动清理自己；在文件重定向时，shell使用完`/tmp/xyz`后必须小心删除
- 其次，管道可以任意传递长的数据流，而文件重定向需要磁盘上足够的空闲空间来存储所有的数据。
- 第三，管道允许并行执行管道阶段，而文件方法要求第一个程序在第二个程序启动之前完成。
- 第四，如果实现进程间通讯，管道的块读写比文件的非块语义更有效率。

## 文件系统

xv6和linux一样，文件系统是一个以根目录为根节点的文件树

/代表的就是根目录.

/a/b/c 这样的路径是从根目录开始的绝对路径

a这样的路径是当前目录下的相对路径

关于文件系统的系统调用:

- mkdir

- open

- mknod

- link

- unlink

- fstat

  

mkdir创建一个新的目录,open中如果使用宏定义O_CRATE会创建一个文件,mknod创建一个新的设备文件

### mknod

`mknod`创建一个引用设备的特殊文件。与设备文件相关联的是主设备号和次设备号(`mknod`的两个参数)，它们唯一地标识了一个内核设备。当进程稍后打开设备文件时，内核将使用内核设备实现`read`和`write`系统调用，而不是使用文件系统。

一个文件的名字和文件本身是不同的;同一个底层文件（叫做inode，索引结点）可以有多个名字（叫做link，链接）。每个链接都由目录中的一个条目组成;该条目包含一个文件名和一个inode引用。Inode保存有关文件的元数据（用于解释或帮助理解信息的数据），包括其类型(文件/目录/设备)、长度、文件内容在磁盘上的位置以及指向文件的链接数。

#### fstat

![image-20221128001006726](https://raw.githubusercontent.com/lozijy/image/main/image-20221128001006726.png)

`fstat`系统调用从文件描述符所引用的inode中检索信息。它填充一个`stat`类型的结构体，`struct stat`在***stat.h(kernel/stat.h)\***中定义为

```c
#define T_DIR 1    // Directory
#define T_FILE 2   // File
#define T_DEVICE 3 // Device
struct stat {
    int dev;     // 文件系统的磁盘设备
    uint ino;    // Inode编号
    short type;  // 文件类型
    short nlink; // 指向文件的链接数
    uint64 size; // 文件字节数
};
```

#### link unlink

`link`系统调用创建另一个文件名，该文件名指向与现有文件相同的inode。下面的代码片段创建了一个名字既为***a\***又为***b\***的新文件

```c
open("a", O_CREATE | O_WRONLY);
link("a", "b");
```

从***a\***读取或写入与从***b\***读取或写入是相同的操作。每个inode由唯一的inode编号标识。在上面的代码序列之后，可以通过检查`fstat`的结果来确定a和b引用相同的底层内容:两者都将返回相同的inode号(`ino`)，并且`nlink`计数将被设置为2。

`unlink`系统调用从文件系统中删除一个名称。只有当文件的链接数为零且没有文件描述符引用时，文件的inode和包含其内容的磁盘空间才会被释放，因此添加

```c
unlink("a");
```

最后一行代码序列中会使inode和文件内容可以作为b访问。此外

```c
fd = open("/tmp/xyz", O_CREATE | O_RDWR);
unlink("/tmp/xyz");
```

是创建没有名称的临时inode的惯用方法，该临时inode将在进程关闭fd或退出时被清理。

## 真实世界

#### POSIX标准

```sh
POSIX 标准定义了操作系统应该为应用程序提供的接口标准，是 IEEE 为要在各种 UNIX 操作系统上运行的软件而定义的一系列 API 标准的总称，其正式称呼为 IEEE 1003，而国际标准名称为 ISO/IEC 9945。POSIX 标准意在期望获得源代码级别的软件可移植性。换句话说，为一个 POSIX 兼容的操作系统编写的程序，应该可以在任何其它的 POSIX 操作系统（即使是来自另一个厂商）上编译执行。

　　POSIX 并不局限于 UNIX。许多其它的操作系统，例如 DEC OpenVMS 支持 POSIX 标准，尤其是 IEEE Std. 1003.1-1990（1995 年修订）或 POSIX.1，POSIX.1 提供了源代码级别的 C 语言应用编程接口（API）给操作系统的服务程序，例如读写文件。POSIX.1 已经被国际标准化组织（International Standards Organization，ISO）所接受，被命名为 ISO/IEC 9945-1:1990 标准。
POSIX.1 仅规定了系统服务应用程序编程接口（API），仅概况了基本的系统服务标准。
Linux 刚刚起步的时候，这个 UNIX 标准为 Linux 提供了极为重要的信息，使得 Linux 能够在标准的指导下进行开发，并能够与绝大多数 UNIX 操作系统兼容。
posix标准的出现就是为了解决这个问题。linux和windows都要实现基本的posix标准，linux把fork函数封装成posix_fork（随便说的），windows把creatprocess函数也封装成posix_fork，都声明在unistd.h里。

这样，程序员编写普通应用时候，只用包含unistd.h，调用posix_fork函数，程序就在源代码级别可移植了。
```



#### Xv6和POSIX不兼容



Unix系统调用接口已经通过便携式操作系统接口(POSIX)标准进行了标准化。Xv6与POSIX不兼容:它缺少许多系统调用(包括lseek等基本系统调用)，并且它提供的许多系统调用与标准不同。我们xv6的主要目标是简单明了，同时提供一个简单的类unix系统调用接口。为了运行基本的Unix程序，有些人扩展了xv6，增加了一些系统调用和一个简单的c库。然而，现代内核比xv6提供了更多的系统调用和更多种类的内核服务。例如，它们支持网络工作、窗口系统、用户级线程、许多设备的驱动程序等等。现代内核不断快速发展，提供了许多超越POSIX的特性。

2.

Xv6没有提供一个用户概念或者保护一个用户不受另一个用户的伤害;用Unix的术语来说，所有的Xv6进程都作为root运行。

本书研究了xv6如何实现其类Unix接口，但这些思想和概念不仅仅适用于Unix。任何操作系统都必须在底层硬件上复用进程，彼此隔离进程，并提供受控制的进程间通讯机制。在学习了xv6之后，你应该去看看更复杂的操作系统，以及这些系统中与xv6相同的底层基本概念。



## 其他自己调用系统接口做的函数实例

1.copy

对read接口的应用

```c
int main(){
    while(1){
        int n=read(0,buf,sizeof(buf));//默认0连接到控制台输入
        if(n<=0){
            break;
        }
        write(1,buf,n);//默认1连接到控制台输出
    }
    
    
}
```

2.open

对open接口的应用

```c
int main(){
    int fd=open("output.txt",O_WRONLY|O_CREATE);
    if(fd!=-1){write(fd,"hello",5)}//"hello是指针返回第一个字符的地址"
}
```

3. exec	

   对exec接口的应用

   ```c
   #include "kernel/types.h"
   #include "user/user.h"
   int main(){
   	char *args[]={"echo","this","is","echo",0};
   	exec("echo",args);
   	exit(0);
       printf("exec failed!\n")
   }
   ```

   

   4.forkexec

   对fork,exec的应用

   exec执行一个文件后,他执行的程序就会替代自身，而且不会返回自身(不会执行接下来的命令)，所以我们常常把fork和exec联合使用，这样就不会对父进程产生影响，但是对于一个比较大的程序来说,fork复制内存，又让它去exec,浪费了很多资源，我们后面会学习一个懒复制的trick..

   ```c
   #include "kernel/types.h"
   #include "user/user.h"
   int main(){
   	int pid,status;    
   	char *args[]={"echo","hello","forkexec",0};
   	pid=fork();
   	if(pid==0){
   	printf("child");//printf is included in the "user.h"
   	exec("echo",args);
       printf("exec failed");//if the line 9 is failed, this line will be executed
   	}else if(pid!=0){
   	printf("parent");	
       wait(&status);
       printf("the child is exit with the status %d",status);
   	}
   	exit(0);
   }
   
   
   ```

   5.redirect

   对close,open的应用

   大概就是close关闭一个文件描述符，再open就会顶替这个文件描述符，从而实现重定向.

   ```c
   #include "kernel/types.h"
   #include "user/user.h"
   
   int main(){
       int pid;
       char *args[]={"echo","hello","redirect",0};
       pid=fork();
       if(pid ==0){
           close(1);//close the current filedescripter,is the "1",not the zero!
           open("output.txt",0x201);//0_WRONLY|0_CREATE,but Ican't find the head file
           exec("echo",args);
       }else{
           wait((int *)0);
       }
       exit(0);
   }
   ```

   



# 第二章 OS Organization and System Calls

操作系统必须满足三个要求：多路复用、隔离和交互。

1.可以通过fork产生进程即使进程比处理器多，我们也要实现并发执行

2.操作系统还必须安排进程之间的隔离。也就是说，如果一个进程有错误和故障，它不应该影响不依赖于有错误的进程的进程

3.完全隔离又太过头了，进程之间应当可以进行刻意为之的交互；管道就是一个例子



Xv6运行在多核RISC-V微处理器上，它的许多低级功能（例如，它的进程实现）是特定于RISC-V的。RISC-V是一个64位的中央处理器，xv6是用基于“LP64”的C语言编写的，这意味着C语言中的`long`（L）和指针（P）变量都是64位的，但`int`是32位的。这本书假设读者已经在一些架构上做了一些机器级编程，并将在出现时介绍RISC-V特定的想法。RISC-V的一个有用的参考文献是《The RISC-V Reader: An Open Architecture Atlas》。用户级ISA和特权指令架构均是官方规范。

### 抽象系统资源

1.将资源抽象为服务

为了让进程之间拥有更好的隔离性(运行时不会相互影响，分时占有cpu资源，在内存中不会访问到其他进程占有的内存)，我们更习惯于禁止应用程序直接访问敏感的硬件，将资源抽象为服务，比如xv6中不允许直接访问磁盘，而是通过调用open,write等系统调用来间接的访问磁盘

2.透明操作

操作系统在进程间透明地切换硬件，保存和恢复寄存器状态，应用程序不会意识到分时共享的存在

一个例子

unix使用exec(系统调用)来构建内存映像，而不是直接与物理内存交互，允许操作系统自行决定进程在内存中的位置，如果内存很紧张，操作系统甚至可以讲一个进程的一些数据暂存在磁盘中

另一个例子

Unix进程之间的许多交互形式都是通过文件描述符实现的。文件描述符不仅抽象了许多细节（例如，管道或文件中的数据存储在哪里），而且还以简化交互的方式进行了定义。例如，如果流水线中的一个应用程序失败了，内核会为流水线中的下一个进程生成文件结束信号（EOF）



系统调用接口是精心设计的，既为程序员提供了便利，又提供了强隔离的可能性。Unix接口不是抽象资源的唯一方法，但它已经被证明是一个非常好的方法

**unix系统调用不是抽象资源的唯一方法，但确实是一个很好的方法**



### 用户态核心态以及系统调用

1.强隔离

应用程序和操作系统间有硬边界，要实现强隔离，操作系统必须保证应用程序不能修改和读取操作系统的数据结构和指令，应用程序不能访问其他进程的内存

2.3种运行模式

用户模式,管理模式，机器模式

机器模式下，指令具有完全特权，XV6下只在机器模式下执行很少的代码便转到了管理模式

管理模式下,cup允许执行特权指令，比如启动和禁止中断，读取和保存页表地址的寄存器。如果用户模式下试图执行特权指令，操作系统会先切换到管理模式再执行指令，RISCV提供了特权指令ecall从用户模式转到管理模式，特权指令sret从内核空间返回到用户空间

处于用户模式下的应用软件被称为在用户空间中运行，处于管理模式下的软件可以执行特权指令，被称为在内核空间中运行,**在内核空间(或管理模式)中运行的软件被称为内核**

3.ecall

想要调用内核函数必须进入内核态(或者叫管理模式，或内核空间),为此riscv-v提供了一个指令ecall，通过调用它并提供相应的参数可以进入到内核态并执行你想执行的内核函数(当然，操作系统会对参数进行检查)

比如说你在用户态下想用系统调用fork,其实你在用户态下并不会直接调用fork，而是先通过ecall(3),进入到内核态，把参数"3"给到内核，内核通过这个"3"就知道了用户态想调用fork,他就会执行真正的fork函数，并把相应结果返回到用户态下。

### 内核组织

1.宏内核和微内核

#### 宏内核:

宏内核是指整个操作系统都在内核模式下运行,这样做的好处是整个操作系统的很多功能，比如说虚拟内存，文件管理，可以相互联系而无需在用户态和内核态间相互切换，大大提高了运行效率，所以说一般的桌面操作系统都是宏内核，比如unix,windows,linux等等，当然坏处也有，那就是bug太难修复了,在内核态下我们会直接对硬件进行操作，要求程序必须是可靠的，如果我们把操作系统所有代码全放在内核态下运行难免会有bug。

#### 微内核

只有极少部分与硬件高度相关的才放进微内核中，其他服务都是在用户空间中运行的

```shell
TIPS

由于客户/服务器（Client/Server）模式，具有非常多的优点，故在单机微内核操作系统中几乎无一例外地都采用客户/服务器模式，将操作系统中最基本的部分放入内核中，而把操作系统的绝大部分功能都放在微内核外面的一组服务器(进程)中实现。

这一点在计算机操作系统第四版(西安电子科技大学出版社)已有所提及
```



宏内核的优点就是微内核的缺点，宏内核的缺点就是微内核的优点.

举个例子

shell(本质上也是一个用户程序)调用exec是如何访问到文件系统FS的?

在微内核下，shell调用exec需要访问到文件系统FS,通常的一个工作模式是通过IPC系统发送一个消息给内核,内核查看，知道需要访问文件系统，发送给文件系统，FS发送一条消息，表明这是EXEC系统调用的结果，再发送会Shell,跳入内核跳入内核，跳出内核，系统调用次数过多，因此微内核很难实现高性能

![image-20221128104021219](https://raw.githubusercontent.com/lozijy/image/main/image-20221128104021219.png)

大多数桌面操作系统都是宏内核系统,很多嵌入式操作系统如MiniX往往是微内核设计

### 代码:XV6架构篇

XV6的源代码位于***kernel/\***子目录中，源代码按照模块化的概念划分为多个文件，图2.2列出了这些文件，模块间的接口都被定义在了***def.h\***（***kernel/defs.h\***）

![image-20221128010817802](https://raw.githubusercontent.com/lozijy/image/main/image-20221128010817802.png)

```c
// bio.c
void            binit(void);
struct buf*     bread(uint, uint);
void            brelse(struct buf*);
void            bwrite(struct buf*);
void            bpin(struct buf*);
void            bunpin(struct buf*);

// console.c
void            consoleinit(void);
void            consoleintr(int);
void            consputc(int);

// exec.c
int             exec(char*, char**);

// file.c
struct file*    filealloc(void);
void            fileclose(struct file*);
struct file*    filedup(struct file*);
void            fileinit(void);
int             fileread(struct file*, uint64, int n);
int             filestat(struct file*, uint64 addr);
int             filewrite(struct file*, uint64, int n);

// fs.c
void            fsinit(int);
int             dirlink(struct inode*, char*, uint);
struct inode*   dirlookup(struct inode*, char*, uint*);
struct inode*   ialloc(uint, short);
struct inode*   idup(struct inode*);
void            iinit();
void            ilock(struct inode*);
void            iput(struct inode*);
void            iunlock(struct inode*);
void            iunlockput(struct inode*);
void            iupdate(struct inode*);
int             namecmp(const char*, const char*);
struct inode*   namei(char*);
struct inode*   nameiparent(char*, char*);
int             readi(struct inode*, int, uint64, uint, uint);
void            stati(struct inode*, struct stat*);
int             writei(struct inode*, int, uint64, uint, uint);
void            itrunc(struct inode*);

// ramdisk.c
void            ramdiskinit(void);
void            ramdiskintr(void);
void            ramdiskrw(struct buf*);

// kalloc.c
void*           kalloc(void);
void            kfree(void *);
void            kinit(void);

// log.c
void            initlog(int, struct superblock*);
void            log_write(struct buf*);
void            begin_op(void);
void            end_op(void);

// pipe.c
int             pipealloc(struct file**, struct file**);
void            pipeclose(struct pipe*, int);
int             piperead(struct pipe*, uint64, int);
int             pipewrite(struct pipe*, uint64, int);

// printf.c
void            printf(char*, ...);
void            panic(char*) __attribute__((noreturn));
void            printfinit(void);

// proc.c
int             cpuid(void);
void            exit(int);
int             fork(void);
int             growproc(int);
void            proc_mapstacks(pagetable_t);
pagetable_t     proc_pagetable(struct proc *);
void            proc_freepagetable(pagetable_t, uint64);
int             kill(int);
struct cpu*     mycpu(void);
struct cpu*     getmycpu(void);
struct proc*    myproc();
void            procinit(void);
void            scheduler(void) __attribute__((noreturn));
void            sched(void);
void            sleep(void*, struct spinlock*);
void            userinit(void);
int             wait(uint64);
void            wakeup(void*);
void            yield(void);
int             either_copyout(int user_dst, uint64 dst, void *src, uint64 len);
int             either_copyin(void *dst, int user_src, uint64 src, uint64 len);
void            procdump(void);

// swtch.S
void            swtch(struct context*, struct context*);

// spinlock.c
void            acquire(struct spinlock*);
int             holding(struct spinlock*);
void            initlock(struct spinlock*, char*);
void            release(struct spinlock*);
void            push_off(void);
void            pop_off(void);

// sleeplock.c
void            acquiresleep(struct sleeplock*);
void            releasesleep(struct sleeplock*);
int             holdingsleep(struct sleeplock*);
void            initsleeplock(struct sleeplock*, char*);

// string.c
int             memcmp(const void*, const void*, uint);
void*           memmove(void*, const void*, uint);
void*           memset(void*, int, uint);
char*           safestrcpy(char*, const char*, int);
int             strlen(const char*);
int             strncmp(const char*, const char*, uint);
char*           strncpy(char*, const char*, int);

// syscall.c
int             argint(int, int*);
int             argstr(int, char*, int);
int             argaddr(int, uint64 *);
int             fetchstr(uint64, char*, int);
int             fetchaddr(uint64, uint64*);
void            syscall();

// trap.c
extern uint     ticks;
void            trapinit(void);
void            trapinithart(void);
extern struct spinlock tickslock;
void            usertrapret(void);

// uart.c
void            uartinit(void);
void            uartintr(void);
void            uartputc(int);
void            uartputc_sync(int);
int             uartgetc(void);

// vm.c
void            kvminit(void);
void            kvminithart(void);
void            kvmmap(pagetable_t, uint64, uint64, uint64, int);
int             mappages(pagetable_t, uint64, uint64, uint64, int);
pagetable_t     uvmcreate(void);
void            uvminit(pagetable_t, uchar *, uint);
uint64          uvmalloc(pagetable_t, uint64, uint64);
uint64          uvmdealloc(pagetable_t, uint64, uint64);
int             uvmcopy(pagetable_t, pagetable_t, uint64);
void            uvmfree(pagetable_t, uint64);
void            uvmunmap(pagetable_t, uint64, uint64, int);
void            uvmclear(pagetable_t, uint64);
uint64          walkaddr(pagetable_t, uint64);
int             copyout(pagetable_t, uint64, char *, uint64);
int             copyin(pagetable_t, char *, uint64, uint64);
int             copyinstr(pagetable_t, char *, uint64, uint64);

// plic.c
void            plicinit(void);
void            plicinithart(void);
int             plic_claim(void);
void            plic_complete(int);

// virtio_disk.c
void            virtio_disk_init(void);
void            virtio_disk_rw(struct buf *, int);
void            virtio_disk_intr(void);

// number of elements in fixed-size array
#define NELEM(x) (sizeof(x)/sizeof((x)[0]))
```

kernel/syscall.h

```c
// System call numbers
#define SYS_fork    1
#define SYS_exit    2
#define SYS_wait    3
#define SYS_pipe    4
#define SYS_read    5
#define SYS_kill    6
#define SYS_exec    7
#define SYS_fstat   8
#define SYS_chdir   9
#define SYS_dup    10
#define SYS_getpid 11
#define SYS_sbrk   12
#define SYS_sleep  13
#define SYS_uptime 14
#define SYS_open   15
#define SYS_write  16
#define SYS_mknod  17
#define SYS_unlink 18
#define SYS_link   19
#define SYS_mkdir  20
#define SYS_close  21

```

kernel/syscall.c

```c
//这里extern只是声明，真正定义在sysfile.c和sysproc.c中
extern uint64 sys_chdir(void);
extern uint64 sys_close(void);
extern uint64 sys_dup(void);
extern uint64 sys_exec(void);
extern uint64 sys_exit(void);
extern uint64 sys_fork(void);
extern uint64 sys_fstat(void);
extern uint64 sys_getpid(void);
extern uint64 sys_kill(void);
extern uint64 sys_link(void);
extern uint64 sys_mkdir(void);
extern uint64 sys_mknod(void);
extern uint64 sys_open(void);
extern uint64 sys_pipe(void);
extern uint64 sys_read(void);
extern uint64 sys_sbrk(void);
extern uint64 sys_sleep(void);
extern uint64 sys_unlink(void);
extern uint64 sys_wait(void);
extern uint64 sys_write(void);
extern uint64 sys_uptime(void);

static uint64 (*syscalls[])(void) = {
[SYS_fork]    sys_fork,
[SYS_exit]    sys_exit,
[SYS_wait]    sys_wait,
[SYS_pipe]    sys_pipe,
[SYS_read]    sys_read,
[SYS_kill]    sys_kill,
[SYS_exec]    sys_exec,
[SYS_fstat]   sys_fstat,
[SYS_chdir]   sys_chdir,
[SYS_dup]     sys_dup,
[SYS_getpid]  sys_getpid,
[SYS_sbrk]    sys_sbrk,
[SYS_sleep]   sys_sleep,
[SYS_uptime]  sys_uptime,
[SYS_open]    sys_open,
[SYS_write]   sys_write,
[SYS_mknod]   sys_mknod,
[SYS_unlink]  sys_unlink,
[SYS_link]    sys_link,
[SYS_mkdir]   sys_mkdir,
[SYS_close]   sys_close,
};

void
syscall(void)
{
    //拿到当前进程中寄存器中的a7(系统调用向量号)，进行是否越界的判断后便进行调用，并把返回参数放进a0
  int num;
  struct proc *p = myproc();

  num = p->trapframe->a7;
  if(num > 0 && num < NELEM(syscalls) && syscalls[num]) {
    p->trapframe->a0 = syscalls[num]();
  } else {
    printf("%d %s: unknown sys call %d\n",
            p->pid, p->name, num);
    p->trapframe->a0 = -1;
  }
}

```

理解:

1. extern

[关键字extern解释](https://www.geeksforgeeks.org/understanding-extern-keyword-in-c/)

2. 函数指针数组

demo:

```c
#define SYS_write 1
extern int sys_write(void);
static int (*syscalls[])(void)={
    [SYS_write] sys_write
};
```

3. struct trapframe 

 p->trapframe

trampoline.S中陷阱处理代码的每个进程数据。单独位于/ user页表中蹦床页的下面。没有特别映射到内核页表中。刮痕寄存器点在这里。在trampoline.S的uservec将用户寄存器保存在trapframe中，然后从trapframe的 kernel_sp, kernel_hartid, kernel_satp中初始化寄存器，然后跳转到kernel_trap。// usertrapret()和userret在trampoline.S设置// trapframe的kernel_*，从/trapframe中恢复用户寄存器，切换到用户页面表，并进入用户空间。

trapframe包含被调用者保存的用户寄存器，如se-s11，因为通过usertrapret()返回到用户的路径没有通过

整个内核调用堆栈。

```c
// per-process data for the trap handling code in trampoline.S.
// sits in a page by itself just under the trampoline page in the
// user page table. not specially mapped in the kernel page table.
// the sscratch register points here.
// uservec in trampoline.S saves user registers in the trapframe,
// then initializes registers from the trapframe's
// kernel_sp, kernel_hartid, kernel_satp, and jumps to kernel_trap.
// usertrapret() and userret in trampoline.S set up
// the trapframe's kernel_*, restore user registers from the
// trapframe, switch to the user page table, and enter user space.
// the trapframe includes callee-saved user registers like s0-s11 because the
// return-to-user path via usertrapret() doesn't return through
// the entire kernel call stack.
struct trapframe {
  /*   0 */ uint64 kernel_satp;   // kernel page table
  /*   8 */ uint64 kernel_sp;     // top of process's kernel stack
  /*  16 */ uint64 kernel_trap;   // usertrap()
  /*  24 */ uint64 epc;           // saved user program counter
  /*  32 */ uint64 kernel_hartid; // saved kernel tp
  /*  40 */ uint64 ra;
  /*  48 */ uint64 sp;
  /*  56 */ uint64 gp;
  /*  64 */ uint64 tp;
  /*  72 */ uint64 t0;
  /*  80 */ uint64 t1;
  /*  88 */ uint64 t2;
  /*  96 */ uint64 s0;
  /* 104 */ uint64 s1;
  /* 112 */ uint64 a0;
  /* 120 */ uint64 a1;
  /* 128 */ uint64 a2;
  /* 136 */ uint64 a3;
  /* 144 */ uint64 a4;
  /* 152 */ uint64 a5;
  /* 160 */ uint64 a6;
  /* 168 */ uint64 a7;
  /* 176 */ uint64 s2;
  /* 184 */ uint64 s3;
  /* 192 */ uint64 s4;
  /* 200 */ uint64 s5;
  /* 208 */ uint64 s6;
  /* 216 */ uint64 s7;
  /* 224 */ uint64 s8;
  /* 232 */ uint64 s9;
  /* 240 */ uint64 s10;
  /* 248 */ uint64 s11;
  /* 256 */ uint64 t3;
  /* 264 */ uint64 t4;
  /* 272 */ uint64 t5;
  /* 280 */ uint64 t6;
};
```

4. proc.c/myproc

```c
// Must be called with interrupts disabled,
// to prevent race with process being moved
// to a different CPU.
int
cpuid()
{
  int id = r_tp();
  return id;
}

// Return this CPU's cpu struct.
// Interrupts must be disabled.
struct cpu*
mycpu(void) {
  int id = cpuid();
  struct cpu *c = &cpus[id];
  return c;
}

struct proc*
myproc(void) {
  push_off();
  struct cpu *c = mycpu();
  struct proc *p = c->proc;
  pop_off();
  return p;
}
```

5. 

sysproc

```c
#include "types.h"
#include "riscv.h"
#include "defs.h"
#include "date.h"
#include "param.h"
#include "memlayout.h"
#include "spinlock.h"
#include "proc.h"

uint64
sys_exit(void)
{
  int n;
  if(argint(0, &n) < 0)
    return -1;
  exit(n);
  return 0;  // not reached
}

uint64
sys_getpid(void)
{
  return myproc()->pid;
}

uint64
sys_fork(void)
{
  return fork();
}

uint64
sys_wait(void)
{
  uint64 p;
  if(argaddr(0, &p) < 0)
    return -1;
  return wait(p);
}

uint64
sys_sbrk(void)
{
  int addr;
  int n;

  if(argint(0, &n) < 0)
    return -1;
  addr = myproc()->sz;
  if(growproc(n) < 0)
    return -1;
  return addr;
}

uint64
sys_sleep(void)
{
  int n;
  uint ticks0;

  if(argint(0, &n) < 0)
    return -1;
  acquire(&tickslock);
  ticks0 = ticks;
  while(ticks - ticks0 < n){
    if(myproc()->killed){
      release(&tickslock);
      return -1;
    }
    sleep(&ticks, &tickslock);
  }
  release(&tickslock);
  return 0;
}

uint64
sys_kill(void)
{
  int pid;

  if(argint(0, &pid) < 0)
    return -1;
  return kill(pid);
}

// return how many clock tick interrupts have occurred
// since start.
uint64
sys_uptime(void)
{
  uint xticks;

  acquire(&tickslock);
  xticks = ticks;
  release(&tickslock);
  return xticks;
}
```

sysfile.c

```c
#include "types.h"
#include "riscv.h"
#include "defs.h"
#include "date.h"
#include "param.h"
#include "memlayout.h"
#include "spinlock.h"
#include "proc.h"

uint64
sys_exit(void)
{
  int n;
  if(argint(0, &n) < 0)
    return -1;
  exit(n);
  return 0;  // not reached
}

uint64
sys_getpid(void)
{
  return myproc()->pid;
}

uint64
sys_fork(void)
{
  return fork();
}

uint64
sys_wait(void)
{
  uint64 p;
  if(argaddr(0, &p) < 0)
    return -1;
  return wait(p);
}

uint64
sys_sbrk(void)
{
  int addr;
  int n;

  if(argint(0, &n) < 0)
    return -1;
  addr = myproc()->sz;
  if(growproc(n) < 0)
    return -1;
  return addr;
}

uint64
sys_sleep(void)
{
  int n;
  uint ticks0;

  if(argint(0, &n) < 0)
    return -1;
  acquire(&tickslock);
  ticks0 = ticks;
  while(ticks - ticks0 < n){
    if(myproc()->killed){
      release(&tickslock);
      return -1;
    }
    sleep(&ticks, &tickslock);
  }
  release(&tickslock);
  return 0;
}

uint64
sys_kill(void)
{
  int pid;

  if(argint(0, &pid) < 0)
    return -1;
  return kill(pid);
}

// return how many clock tick interrupts have occurred
// since start.
uint64
sys_uptime(void)
{
  uint xticks;

  acquire(&tickslock);
  xticks = ticks;
  release(&tickslock);
  return xticks;
}

```





#### 启动相关:

entry.S

start.c

#### 相关函数

printf.c

exec.c



#### 进程:

proc.c

中断:

plic.c RISCV的中断控制器

trap.c 控制陷入和中断的返回

内核态，用户态切换:

trampoline.S

进程通信:

pipe.c

锁:

spinlock.c

sleeplock.c



#### 内存:

kalloc物理页分配器

页表:

vm.c管理页表和地址空间

#### 文件:

bio.c

sysfile.c

#### 磁盘:



1

.文件结构

有这些文件夹

kernel 内核

user 用户

mkfs 

makefile 用来对整个项目进行编译的文件，注意做lab的时候要修改一下这个文件 

产生了 kernel/kernel.asm可以用来debug.看到反汇编指令

qemu和内核约定任何程序的起始点是地址80000000

面对qemu我们不能把它想象成一个C程序，而是真正想象成一块电路板

### 进程概述

1.进程

xv6和其他unix操作系统一样，隔离单位是一个进程，每个进程在内存中有独特的物理空间，禁止在内存中访问其他进程。为了加强隔离性，进程抽象给每个进程一个错觉,它们拥有自己的专有机器，进程为程序提供了一个像是私有内存系统和地址空间的东西,其他进程不能读取或者写入

xv6使用页表为每个进程提供自己的地址空间，risc-v页表将虚拟地址转换为物理地址(需要注意的是,在每个进程中，执行程序时会用虚拟地址，但在进程和操作系统的交互时，提供的是物理地址(不然不知道进程到底在哪))

2.地址空间

![image-20221128011343123](https://raw.githubusercontent.com/lozijy/image/main/image-20221128011343123.png)

页表定义了进程的地址空间

指令-全局变量-栈区-堆区

地址空间的最大范围:

RISC-V上的指针有64位宽，硬件在页表中查找虚拟地址时只需要用到低39位，xv6只使用者39位的38位。

在地址空间顶部，分别为trampolineHe 和trapframe保留了一个页面

3.状态片段

![image-20221128104346878](https://raw.githubusercontent.com/lozijy/image/main/image-20221128104346878.png)

每个进程维护了很多状态片段，聚集在proc结构体中,一个进程最重要的内核状态片段就是 页表，内核栈区和运行状态,可以用p->xxx引用proc结构体的元素.

4.线程:

每个进程可以有一个线程或多个线程执行进程的指令，一个线程可以挂起然后恢复,,挂起时，数据存储在进程的栈区里。

线程大部分中状态储存在线程的栈区上，每个进程有两个栈区:一个用户栈区和一个内核栈区,进程进入内核实现系统调用或中断时内核代码在内核栈区上运行,一个进程在内核态时用户栈区仍然保留数据，但处于不活跃的状态

5.再谈ecall指令

一个进程可以执行ecall指令进行系统调用，提升硬件特权级别,并将PC更改为内核定义的入口点。入口点的代码切换到内核栈，执行实现系统调用的内核指令，当系统调用完成时，内核切换回用户栈,并通过sret指令返回用户空间，降低硬件的特权级别，并在系统调用完成时恢复执行用户指令。进程的线程可以在内核中阻塞等待I/O，并在I/O完成后恢复到中断的位置。

### 代码:启动XV6和第一个进程

#### 通过gdb进行debug调试看看操作系统启动发生了什么?

让我们的qemu支持gdb调试工具

```shell
make CPUS=1 qemu-gdb

*** Now run 'gdb' in another window.

```

开启gdb

```shell
gdb-multiarch
```

或者

```shell
riscv64-linux-gnu-gdb
```

看你下载的那个调试工具

multiarch提供的一些指令



#### entry.s

kernel/entry.s

cpu从_entry(kernel/entry.S)开始执行,这时分页处于禁用状态,虚拟地址直接映射到物理地址



加载程序将xv6内核加载到物理地址为0x80000000的内存中，它将内核放在0x80000000而不是0x0的原因是地址范围0x0:0x80000000包含I/0设备

_entry设置了一个栈区,这样xv6就可以执行c代码,栈的声明在start.c中



栈是从高地址向低地址增长的，所以我们把栈顶位置设置在stack0+4096的位置，加载到寄存器sp中,有了栈我们就可以执行start函数

#### start函数:

kernel/start.c

```c
void
start()
{
  // set M Previous Privilege mode to Supervisor, for mret.
  unsigned long x = r_mstatus();//start函数在寄存器mstatus中将先前的运行模式改成管理模式，
  x &= ~MSTATUS_MPP_MASK;
  x |= MSTATUS_MPP_S;
  w_mstatus(x);

  // set M Exception Program Counter to main, for mret.
  // requires gcc -mcmodel=medany
  w_mepc((uint64)main);//通过将main函数的地址写进寄存器mepc,将返回地址设为main

  // disable paging for now.
  w_satp(0);//通过向页表寄存器satp写入0在管理模式下禁用虚拟地址转换

  // delegate all interrupts and exceptions to supervisor mode.
  w_medeleg(0xffff);
  w_mideleg(0xffff);
  w_sie(r_sie() | SIE_SEIE | SIE_STIE | SIE_SSIE);//给管理者模式所有的中断和异常授权

  // configure Physical Memory Protection to give supervisor mode
  // access to all of physical memory.
  w_pmpaddr0(0x3fffffffffffffull);//设置物理内存保护来给予管理者模式
  w_pmpcfg0(0xf);

  // ask for clock interrupts.
  timerinit();//申请时钟中断

  // keep each CPU's hartid in its tp register, for cpuid().
  int id = r_mhartid();
  w_tp(id);

  // switch to supervisor mode and jump to main().
  asm volatile("mret");//转换到管理者模式并跳到main函数
}
```

start函数在寄存器mstatus中将先前的运行模式改成管理模式，

寄存器 在mstatus中将运行模式改成管理模式,将main地址写进mepc,页表寄存器satp写入0禁用虚拟内存转换

riscv.h

```c
static inline void 
w_mstatus(uint64 x)
{
  asm volatile("csrw mstatus, %0" : : "r" (x));
}
```



通过将main函数的地址写进寄存器mepc，将返回地址设为main

```c
static inline void 
w_mepc(uint64 x)
{
  asm volatile("csrw mepc, %0" : : "r" (x));
}
```

通过向页表寄存器satp写入0在管理模式下禁用虚拟地址转换

```c
static inline void 
w_satp(uint64 x)
{
  asm volatile("csrw satp, %0" : : "r" (x));
}
```

进入管理模式前,start还要对时钟芯片进行编程以产生计时器中断，做完这些后调用mret这个riscv指令返回到管理模式，这将导致pc的值更改为main函数地址

*引用:一个进程可以执行ecall指令进行系统调用，提升硬件特权级别,并将PC更改为内核定义的入口点。*

```c
  // ask for clock interrupts.
  timerinit();
```

```c
  // switch to supervisor mode and jump to main().
  asm volatile("mret");
```

#### main函数: 

kernel/main.c

```c
#include "types.h"
#include "param.h"
#include "memlayout.h"
#include "riscv.h"
#include "defs.h"

volatile static int started = 0;

// start() jumps here in supervisor mode on all CPUs.
void
main()
{
  if(cpuid() == 0){
    consoleinit();
    printfinit();
    printf("\n");
    printf("xv6 kernel is booting\n");
    printf("\n");
    kinit();         // physical page allocator物理页面管理
    kvminit();       // create kernel page table创建内核页表
    kvminithart();   // turn on paging开启分页
    procinit();      // process table进程表
    trapinit();      // trap vectors陷入向量
    trapinithart();  // install kernel trap vector下载内核陷入向量
    plicinit();      // set up interrupt controller建立中断控制器
    plicinithart();  // ask PLIC for device interrupts向PLIC请求设备中断
    binit();         // buffer cache缓存
    iinit();         // inode table文件inode表
    fileinit();      // file table文件表
    virtio_disk_init(); // emulated hard disk
    
    userinit();      // first user process创建第一个用户进程并进入用户空间
    __sync_synchronize();
    started = 1;
  } else {
    while(started == 0)
      ;
    __sync_synchronize();
    printf("hart %d starting\n", cpuid());
    kvminithart();    // turn on paging开启分页
    trapinithart();   // install kernel trap vector下载内核陷入向量
    plicinithart();   // ask PLIC for device interrupts向PLIC请求设备中断
  }

  scheduler();        
}
```

main函数会初始化几个设备和子系统，便通过调用userinit(kernel/proc.c)创建第一个进程，进入用户空间

```c
uchar initcode[] = {
  0x17, 0x05, 0x00, 0x00, 0x13, 0x05, 0x45, 0x02,
  0x97, 0x05, 0x00, 0x00, 0x93, 0x85, 0x35, 0x02,
  0x93, 0x08, 0x70, 0x00, 0x73, 0x00, 0x00, 0x00,
  0x93, 0x08, 0x20, 0x00, 0x73, 0x00, 0x00, 0x00,
  0xef, 0xf0, 0x9f, 0xff, 0x2f, 0x69, 0x6e, 0x69,
  0x74, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00,
  0x00, 0x00, 0x00, 0x00
};

// Set up first user process.
void
userinit(void)
{
  struct proc *p;

  p = allocproc();
  initproc = p;
  //分配用户表，并把指令和数据复制进去
  // allocate one user page and copy init's instructions
  // and data into it.
  uvminit(p->pagetable, initcode, sizeof(initcode));
  p->sz = PGSIZE;
 //为第一次从内核空间返回到用户空间做准备
  // prepare for the very first "return" from kernel to user.
  p->trapframe->epc = 0;      // user program counter用户程序计数器
  p->trapframe->sp = PGSIZE;  // user stack pointer用户栈指针

  safestrcpy(p->name, "initcode", sizeof(p->name));
  p->cwd = namei("/");//p的当前工作目录是根目录

  p->state = RUNNABLE;//p状态是可运行

  release(&p->lock);//释放锁
}
```

第一个进程执行一个用RISCV指令集写的小型程序initcode.S(user/initcode.S)，(这3条或4条指令，返回内核空间)

```assembly
# Initial process that execs /init.
# This code runs in user space.

#include "syscall.h"

# exec(init, argv)
.globl start
start:
        la a0, init
        la a1, argv
        li a7, SYS_exec//调用exec进入内核
        ecall	//ecall+SYS_exec

# for(;;) exit();
exit:
        li a7, SYS_exit
        ecall
        jal exit

# char init[] = "/init\0";
init:
  .string "/init\0"

# char *argv[] = { init, 0 };
.p2align 2
argv:
  .long init
  .long 0
```



它通过调用exec系统调用重新进入内核，用一个新的进程替代当前进程的内存和寄存器，调用syscall函数

kernel/syscall

```c
syscall(void)
{
  int num;
  struct proc *p = myproc();

  num = p->trapframe->a7;
  if(num > 0 && num < NELEM(syscalls) && syscalls[num]) {
    p->trapframe->a0 = syscalls[num]();
  } else {
    printf("%d %s: unknown sys call %d\n",
            p->pid, p->name, num);
    p->trapframe->a0 = -1;
  }
}
```

这里的num是7，查看syscall.h发现7是系统调用sys_exec

然后我们就到了sys_exec,他在kernel/sysfile文件里

```c
sys_exec(void)
{
  char path[MAXPATH], *argv[MAXARG];
  int i;
  uint64 uargv, uarg;

  if(argstr(0, path, MAXPATH) < 0 || argaddr(1, &uargv) < 0){
    return -1;
  }
  memset(argv, 0, sizeof(argv));
  for(i=0;; i++){
    if(i >= NELEM(argv)){
      goto bad;
    }
    if(fetchaddr(uargv+sizeof(uint64)*i, (uint64*)&uarg) < 0){
      goto bad;
    }
    if(uarg == 0){
      argv[i] = 0;
      break;
    }
    argv[i] = kalloc();
    if(argv[i] == 0)
      goto bad;
    if(fetchstr(uarg, argv[i], PGSIZE) < 0)
      goto bad;
  }
```

这个函数首先从用户空间获取参数,获取路径名

```c
if(argstr(0, path, MAXPATH) < 0 || argaddr(1, &uargv) < 0)
```

注意这一行

```c
  memset(argv, 0, sizeof(argv));
```

memset为参数分配空间，将所有参数从用户空间复制到内核空间，后面我们会见到更多的细节，基本上是一些代码把参数从用户空间移动到内核空间



一旦完成exec，他就返回／init(user/init.c)进程中的用户空间

```c
// init: The initial user-level program

#include "kernel/types.h"
#include "kernel/stat.h"
#include "kernel/spinlock.h"
#include "kernel/sleeplock.h"
#include "kernel/fs.h"
#include "kernel/file.h"
#include "user/user.h"
#include "kernel/fcntl.h"

char *argv[] = { "sh", 0 };

int
main(void)
{
  int pid, wpid;

  if(open("console", O_RDWR) < 0){
    mknod("console", CONSOLE, 0);
    open("console", O_RDWR);
  }
  dup(0);  // stdout
  dup(0);  // stderr

  for(;;){
    printf("init: starting sh\n");
    pid = fork();
    if(pid < 0){
      printf("init: fork failed\n");
      exit(1);
    }
    if(pid == 0){
      exec("sh", argv);
      printf("init: exec sh failed\n");
      exit(1);
    }

    for(;;){
      // this call to wait() returns if the shell exits,
      // or if a parentless process exits.
      wpid = wait((int *) 0);
      if(wpid == pid){
        // the shell exited; restart it.
        break;
      } else if(wpid < 0){
        printf("init: wait returned an error\n");
        exit(1);
      } else {
        // it was a parentless process; do nothing.
      }
    }
  }
}

```

init基本上就是为用户空间设置一些东西，打开console,console的文件描述符，复制几次，调用fork

然后创建一个进程，然后exec shell

对对对，xv6就这样开始运行啦



### 思维导图

# 第三章 Page tables



### 分页硬件

分页硬件通过使用虚拟地址39位中的前27位索引页表，以找到该虚拟地址对应的一个PTE，然后生成一个56位的物理地址，其前44位来自PTE中的PPN，其后12位来自原始虚拟地址。

![img](http://xv6.dgs.zone/tranlate_books/book-riscv-rev1/images/c3/p1.png)

实际的转换分三个步骤进行。页表以三级的树型结构存储在物理内存中。该树的根是一个4096字节的页表页，其中包含512个PTE，每个PTE中包含该树下一级页表页的物理地址。这些页中的每一个PTE都包含该树最后一级的512个PTE（也就是说每个PTE占8个字节，正如图3.2最下面所描绘的）。分页硬件使用27位中的前9位在根页表页面中选择PTE，中间9位在树的下一级页表页面中选择PTE，最后9位选择最终的PTE。

![img](http://xv6.dgs.zone/tranlate_books/book-riscv-rev1/images/c3/p2.png)



如果转换地址所需的三个PTE中的任何一个不存在，页式硬件就会引发页面故障异常（page-fault exception），并让内核来处理该异常（参见第4章）。

与图 3.1 的单级设计相比，图 3.2 的三级结构使用了一种更节省内存的方式来记录  PTE。在大范围的虚拟地址没有被映射的常见情况下，三级结构可以忽略整个页面目录。举个例子，如果一个应用程序只使用了一个页面，那么顶级页面目录将只使用条目0，条目 1 到 511 都将被忽略，因此内核不必为这511个条目所对应的中间页面目录分配页面，也就更不必为这 511 个中间页目录分配底层页目录的页。 所以，在这个例子中，三级设计仅使用了三个页面。

每个PTE包含标志位，这些标志位告诉分页硬件允许如何使用关联的虚拟地址。`PTE_V`指示PTE是否存在：如果它没有被设置，对页面的引用会导致异常（即不允许）。`PTE_R`控制是否允许指令读取到页面。`PTE_W`控制是否允许指令写入到页面。`PTE_X`控制CPU是否可以将页面内容解释为指令并执行它们。`PTE_U`控制用户模式下的指令是否被允许访问页面；如果没有设置`PTE_U`，PTE只能在管理模式下使用

为了告诉硬件使用页表，内核必须将根页表页的物理地址写入到`satp`寄存器中（`satp`的作用是存放根页表页在物理内存中的地址）。每个CPU都有自己的`satp`，一个CPU将使用自己的`satp`指向的页表转换后续指令生成的所有地址。每个CPU都有自己的`satp`，因此不同的CPU就可以运行不同的进程，每个进程都有自己的页表描述的私有地址空间。



### 内核地址空间

Xv6为每个进程维护一个页面，用来描述每个进程的用户地址空间，外加单独描述内核地址空间的内核页表

#### 内核布局

下图显示了这种布局如何将内核虚拟地址映射到物理地址

![image-20221128154151454](https://raw.githubusercontent.com/lozijy/image/main/image-20221128154151454.png)

QEMU模拟了一台计算机，它包括从物理地址`0x80000000`开始并至少到`0x86400000`结束的RAM（物理内存），xv6称结束地址为`PHYSTOP`

内核使用“直接映射”获取内存和内存映射设备寄存器；也就是说，将资源映射到等于物理地址的虚拟地址。例如，内核本身在虚拟地址空间和物理内存中都位于`KERNBASE=0x80000000`。直接映射简化了读取或写入物理内存的内核代码。例如，当`fork`为子进程分配用户内存时，分配器返回该内存的物理地址；`fork`在将父进程的用户内存复制到子进程时直接将该地址用作虚拟地址。

虽然内核通过高地址内存映射使用内核栈，是它们也可以通过直接映射的地址进入内核。另一种设计可能只有直接映射，并在直接映射的地址使用栈。然而，在这种安排中，提供保护页将涉及取消映射虚拟地址，否则虚拟地址将引用物理内存，这将很难使用。

### 代码:创建一个地址空间

​	def.h:

```c
// vm.c
void            kvminit(void);
void            kvminithart(void);
void            kvmmap(pagetable_t, uint64, uint64, uint64, int);
int             mappages(pagetable_t, uint64, uint64, uint64, int);
pagetable_t     uvmcreate(void);
void            uvminit(pagetable_t, uchar *, uint);
uint64          uvmalloc(pagetable_t, uint64, uint64);
uint64          uvmdealloc(pagetable_t, uint64, uint64);
int             uvmcopy(pagetable_t, pagetable_t, uint64);
void            uvmfree(pagetable_t, uint64);
void            uvmunmap(pagetable_t, uint64, uint64, int);
void            uvmclear(pagetable_t, uint64);
uint64          walkaddr(pagetable_t, uint64);
int             copyout(pagetable_t, uint64, char *, uint64);
int             copyin(pagetable_t, char *, uint64, uint64);
int             copyinstr(pagetable_t, char *, uint64, uint64);

```



大多数用于操作地址空间和页表的xv6代码都写在 ***vm.c\*** ([kernel/vm.c:1](https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L1)) 中。其核心数据结构是`pagetable_t`，它实际上是指向RISC-V根页表页的指针；一个`pagetable_t`可以是内核页表，也可以是一个进程页表。最核心的函数是`walk`和`mappages`，前者为虚拟地址找到PTE，后者为新映射装载PTE。名称以`kvm`开头的函数操作内核页表；以`uvm`开头的函数操作用户页表；其他函数用于二者。`copyout`和`copyin`复制数据到用户虚拟地址或从用户虚拟地址复制数据，这些虚拟地址作为系统调用参数提供; 由于它们需要显式地翻译这些地址，以便找到相应的物理内存，故将它们写在***vm.c\***中。

pagetable_t

```c
typedef uint64 *pagetable_t; 
```

PTE2PA根据PTE找到下一级页表

PA2PTE

walk 为虚拟地址找到PTE,如果原来不存在页表，就会在物理空间中分配一块区域，把对应PTE放进去

```c
      memset(pagetable, 0, PGSIZE);
      *pte = PA2PTE(pagetable) | PTE_V;
```

参数:页表指针,虚拟地址,alloc

返回:如果分配物理页失败就返回0

for level 表示共有三级页表

PX函数估计就是就是一个根据level等级取出虚拟地址前多少位的操作

PTE2PA就是根据PTE找到在物理内存中找到下一级页表

```c
pte_t *
walk(pagetable_t pagetable, uint64 va, int alloc)
{
  if(va >= MAXVA)
    panic("walk");

  for(int level = 2; level > 0; level--) {
    pte_t *pte = &pagetable[PX(level, va)];
    if(*pte & PTE_V) {
      pagetable = (pagetable_t)PTE2PA(*pte);
    } else {
      if(!alloc || (pagetable = (pde_t*)kalloc()) == 0)
        return 0;
      memset(pagetable, 0, PGSIZE);
      *pte = PA2PTE(pagetable) | PTE_V;
    }
  }
  return &pagetable[PX(0, va)];
}

```



#### kvminit

在启动序列的前期，`main` 调用 `kvminit` (***kernel/vm.c\***:54) 以使用 `kvmmake` (***kernel/vm.c\***:20) 创建内核的页表。此调用发生在 xv6 启用 RISC-V 上的分页之前，因此地址直接引用物理内存。**

```c
void
kvminit(void)
{
  kernel_pagetable = kvmmake();
}
```

#### kvmmake

分配一个物理内存页来保存根页表

`kvmmake` 首先分配一个物理内存页来保存根页表页。

string.c/memset: 

传入空指针

```c
void*
memset(void *dst, int c, uint n)
{
  char *cdst = (char *) dst;
  int i;
  for(i = 0; i < n; i++){
    cdst[i] = c;
  }
  return dst;
}
```



```c
pagetable_t
kvmmake(void)
{
  pagetable_t kpgtbl;

  kpgtbl = (pagetable_t) kalloc();
  memset(kpgtbl, 0, PGSIZE);

  // uart registers
  kvmmap(kpgtbl, UART0, UART0, PGSIZE, PTE_R | PTE_W);

  // virtio mmio disk interface 磁盘接口
  kvmmap(kpgtbl, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W);

  // PLIC 映射
  kvmmap(kpgtbl, PLIC, PLIC, 0x400000, PTE_R | PTE_W);

  // map kernel text executable and read-only.
  kvmmap(kpgtbl, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X);

  // map kernel data and the physical RAM we'll make use of.映射内核数据和物理内存
  kvmmap(kpgtbl, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W);

  // map the trampoline for trap entry/exit to映射进入或退出的trampoline内核中的最高虚拟地址
  // the highest virtual address in the kernel.
  kvmmap(kpgtbl, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X);

  // map kernel stacks
  proc_mapstacks(kpgtbl);
  
  return kpgtbl;
}
```

#### kvmmap

装载内核需要的转换:内核的指令和数据，物理内存的上限到PHYSTOP

然后它调用`kvmmap`来装载内核需要的转换。转换包括内核的指令和数据、物理内存的上限到 `PHYSTOP`，并包括实际上是设备的内存。

 `Proc_mapstacks` (***kernel/proc.c\***:33) 为每个进程分配一个内核堆栈。它调用 kvmmap 将每个堆栈映射到由 KSTACK 生成的虚拟地址，从而为无效的堆栈保护页面留出空间。

```c
// add a mapping to the kernel page table.
// only used when booting.
// does not flush TLB or enable paging.
void
kvmmap(pagetable_t kpgtbl, uint64 va, uint64 pa, uint64 sz, int perm)
{
  if(mappages(kpgtbl, va, sz, pa, perm) != 0)
    panic("kvmmap");
}
```



#### mappages

为范围内的每个虚拟地址执行操作，把虚拟地址到物理地址的映射装载到一个页表内

`kvmmap`(***kernel/vm.c\***:127)调用`mappages`(***kernel/vm.c\***:138)，`mappages`将范围虚拟地址到同等范围物理地址的映射装载到一个页表中。它以页面大小为间隔，为范围内的每个虚拟地址单独执行此操作。对于要映射的每个虚拟地址，`mappages`调用`walk`来查找该地址的PTE地址。然后，它初始化PTE以保存相关的物理页号、所需权限（`PTE_W`、`PTE_X`和/或`PTE_R`）以及用于标记PTE有效的`PTE_V`(***kernel/vm.c\***:153)。

```c
// Create PTEs for virtual addresses starting at va that refer to
// physical addresses starting at pa. va and size might not
// be page-aligned. Returns 0 on success, -1 if walk() couldn't
// allocate a needed page-table page.
int
mappages(pagetable_t pagetable, uint64 va, uint64 size, uint64 pa, int perm)
{
  uint64 a, last;
  pte_t *pte;

  if(size == 0)
    panic("mappages: size");
  
  a = PGROUNDDOWN(va);
  last = PGROUNDDOWN(va + size - 1);
  for(;;){
    if((pte = walk(pagetable, a, 1)) == 0)
      return -1;
    if(*pte & PTE_V)
      panic("mappages: remap");
    *pte = PA2PTE(pa) | perm | PTE_V;
    if(a == last)
      break;
    a += PGSIZE;
    pa += PGSIZE;
  }
  return 0;
}
```



#### walk

使用上一级的9位虚拟地址来找到下一级页面或最终页面的PTE,如果设置了alloc参数，情况则有所不同

在查找PTE中的虚拟地址（参见图3.2）时，`walk`(***kernel/vm.c\***:72)模仿RISC-V分页硬件。`walk`一次从3级页表中获取9个比特位。它使用上一级的9位虚拟地址来查找下一级页表或最终页面的PTE (***kernel/vm.c\***:78)。如果PTE无效，则所需的页面还没有分配；如果设置了`alloc`参数，`walk`就会分配一个新的页表页面，并将其物理地址放在PTE中。它返回树中最低一级的PTE地址(***kernel/vm.c\***:88)。

上面的代码依赖于直接映射到内核虚拟地址空间中的物理内存。例如，当`walk`降低页表的级别时，它从PTE (***kernel/vm.c\***:80)中提取下一级页表的（物理）地址，然后使用该地址作为虚拟地址来获取下一级的PTE (***kernel/vm.c\***:78)。

#### kvminitart

安装内核页表将根页表页的物理地址写进寄存器satp

`main`调用`kvminithart` (***kernel/vm.c\***:53)来安装内核页表。它将根页表页的物理地址写入寄存器`satp`。之后，CPU将使用内核页表转换地址。由于内核使用标识映射，下一条指令的当前虚拟地址将映射到正确的物理内存地址。

```c
// Initialize the one kernel_pagetable
void
kvminit(void)
{
  kernel_pagetable = kvmmake();
}

// Switch h/w page table register to the kernel's page table,
// and enable paging.
void
kvminithart()
{
  w_satp(MAKE_SATP(kernel_pagetable));
  sfence_vma();
}
```

#### procinit 

为每个进程分配一个内核栈

`main`中调用的`procinit` (***kernel/proc.c\***:26)为每个进程分配一个内核栈。它将每个栈映射到`KSTACK`生成的虚拟地址，这为无效的栈保护页面留下了空间。`kvmmap`将映射的PTE添加到内核页表中，对`kvminithart`的调用将内核页表重新加载到`satp`中，以便硬件知道新的PTE。

```c
struct proc proc[NPROC];
// initialize the proc table at boot time.
void
procinit(void)
{
  struct proc *p;
  
  initlock(&pid_lock, "nextpid");
  initlock(&wait_lock, "wait_lock");
  for(p = proc; p < &proc[NPROC]; p++) {
      initlock(&p->lock, "proc");
      p->kstack = KSTACK((int) (p - proc));
  }
}

```

#### 快表TLB

每个RISC-V CPU都将页表条目缓存在转译后备缓冲器（快表/TLB）中，当xv6更改页表时，它必须告诉CPU使相应的缓存TLB条目无效。如果没有这么做，那么在某个时候TLB可能会使用旧的缓存映射，指向一个在此期间已分配给另一个进程的物理页面，这样会导致一个进程可能能够在其他进程的内存上涂鸦。

sfence.vma刷新当前CPU的TLBxv6在重新加载`satp`寄存器后，在`kvminithart`中执行`sfence.vma`，并在返回用户空间之前在用于切换至一个用户页表的`trampoline`代码中执行`sfence.vma` (***kernel/trampoline.S\***:79)。



### 物理内存分配

内核必须在运行时为页表、用户内存、内核栈和管道缓冲区分配和释放物理内存。xv6使用内核末尾到`PHYSTOP`之间的物理内存进行运行时分配。它一次分配和释放整个4096字节的页面。它使用链表的数据结构将空闲页面记录下来。分配时需要从链表中删除页面；释放时需要将释放的页面添加到链表中。

### 代码:物理内存分配

#### allocator



分配器(allocator)位于***kalloc.c\***(***kernel/kalloc.c\***:1)中。分配器的数据结构是可供分配的物理内存页的空闲列表。每个空闲页的列表元素是一个`struct run`(***kernel/kalloc.c\***:17)。分配器从哪里获得内存来填充该数据结构呢？它将每个空闲页的`run`结构存储在空闲页本身，因为在那里没有存储其他东西。空闲列表受到自旋锁（spin lock）的保护(***kernel/kalloc.c\***:21-24)。列表和锁被封装在一个结构体中，以明确锁在结构体中保护的字段。现在，忽略锁以及对`acquire`和`release`的调用；第6章将详细查看有关锁的细节。



#### 互斥锁和自旋锁

对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，"自旋"一词就是因此而得名。

自旋锁比较适用于锁使用者保持锁时间比较短的情况。正是由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于互斥锁。

#### kinit

`main`函数调用`kinit`(***kernel/kalloc.c\***:27)来初始化分配器。`kinit`初始化空闲列表以保存从内核结束到`PHYSTOP`之间的每一页。xv6应该通过解析硬件提供的配置信息来确定有多少物理内存可用。然而，xv6假设机器有128兆字节的RAM。

#### freerange,kfree,PGROUNDUP

`kinit`调用`freerange`将内存添加到空闲列表中，在`freerange`中每页都会调用`kfree`。PTE只能引用在4096字节边界上对齐的物理地址（是4096的倍数），所以`freerange`使用`PGROUNDUP`来确保它只释放对齐的物理地址。分配器开始时没有内存；这些对`kfree`的调用给了它一些管理空间。

分配器有时将地址视为整数，以便对其执行算术运算（例如，在`freerange`中遍历所有页面），有时将地址用作读写内存的指针（例如，操纵存储在每个页面中的`run`结构）；这种地址的双重用途是分配器代码充满C类型转换的主要原因。另一个原因是释放和分配从本质上改变了内存的类型。

函数`kfree` (***kernel/kalloc.c\***:47)首先将内存中的每一个字节设置为1。这将导致使用释放后的内存的代码（使用“悬空引用”）读取到垃圾信息而不是旧的有效内容，从而希望这样的代码更快崩溃。然后`kfree`将页面前置（头插法）到空闲列表中：它将`pa`转换为一个指向`struct run`的指针`r`，在`r->next`中记录空闲列表的旧开始，并将空闲列表设置为等于`r`。

#### kalloc

`kalloc`删除并返回空闲列表中的第一个元素。



### 进程地址空间

每个进程都有一个单独的页表，当xv6在进程之间切换时，也会更改页表。如图2.3所示，一个进程的用户内存从虚拟地址零开始，可以增长到MAXVA (***kernel/riscv.h\***:348)，原则上允许一个进程内存寻址空间为256G。



Layout of a process's virtual address space

![image-20221205195416419](https://raw.githubusercontent.com/lozijy/image/main/image-20221205195416419.png)



### 代码:sbrk

### 代码:exec

进程在内存中不能访问其他其他进程的内存

mmu将虚拟地址转化为物理地址

寄存器satp保存着页表的位置，cpu告诉内存管理单元mmu到哪里寻找把虚拟地址转换为物理地址的映射，mmu不保存映射只做转换，

每个进程拥有一个自己特有的页表，当进程切换时，它也会切换satp寄存器的内容。

每个进程的satp寄存器的值是由内核保存的，写入或读取是一个特权指令，只有内核模式的代码能够修改它。

画出来的像是每个虚拟地址都有都在映射中有一个条目，如果这么做的话会有2^64大，仅仅包含这张表就会使用掉所有的内存，这肯定是不现实的。

其实不是对每个虚拟地址有一个映射，而是对每个页面有一个映射，在RSCV中，一个页是4KB，也就是4096字节。这是习惯决定的，几乎所有的处理器页面大小使用4KB，或者支持4KB



虚拟地址分为两部分:索引和偏移量，MMU做转换时使用索引在映射中找到页表编号，页表编号指向4096字节中的一些物理页，再通过偏移量12知道那个页的12个条目被使用

虚拟地址的最高25位没有使用，限制了虚拟地址空间，虚拟空间地址只有64-25=39位大概512G，索引还剩下了39位，27位作为索引，12位作为偏移地址，必须是12，因为2^12=4096，这样才能在页表中找到条目

物理页面的4096字节是连续的在物理地址中，因为映射是以4096字节为粒度的

物理内存56位，这是设计师决定的，所以说物理内存比虚拟内存大(条目44位physical+12位偏移量)

真正的RISCV页表结构:多级结构

顶部9位作为顶层页表目录的索引，4096字节和页面大小一样



# 第四章 Traps and system calls

本书使用陷阱（trap）作为这些情况的通用术语。通常，陷阱发生时正在执行的任何代码都需要稍后恢复，并且不需要意识到发生了任何特殊的事情。也就是说，我们经常希望陷阱是透明的；这对于中断尤其重要，中断代码通常难以预料。通常的顺序是陷阱强制将控制权转移到内核；内核保存寄存器和其他状态，以便可以恢复执行；内核执行适当的处理程序代码（例如，系统调用接口或设备驱动程序）；内核恢复保存的状态并从陷阱中返回；原始代码从它停止的地方恢复。

xv6内核处理所有陷阱。这对于系统调用来说是顺理成章的。由于隔离性要求用户进程不直接使用设备，而且只有内核具有设备处理所需的状态，因而对中断也是有意义的。因为xv6通过杀死违规程序来响应用户空间中的所有异常，它也对异常有意义。

Xv6陷阱处理分为四个阶段： RISC-V CPU采取的硬件操作、为内核C代码执行而准备的汇编程序集“向量”、决定如何处理陷阱的C陷阱处理程序以及系统调用或设备驱动程序服务例程。虽然三种陷阱类型之间的共性表明内核可以用一个代码路径处理所有陷阱，但对于三种不同的情况：来自用户空间的陷阱、来自内核空间的陷阱和定时器中断，分别使用单独的程序集向量和C陷阱处理程序更加方便。

# 第六章 Interrupts and device drivers

# 第七章 Locking

# 第八章 Scheduling

# 第九章 File system

# 第十章 Concurrency revisited

# 第十一章 Summary



1. git 使用

git clone

2.github使用

