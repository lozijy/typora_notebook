### 集成学习

三个核心思想: bagging boosting stacking

### bagging

即bootstrap aggregating bootstrap是有放回的抽样方法,原理是把多个基础模型放在一起，然后求平均值，这里把决策树作为基础模型。

公式:



首先对数据集进行随机采样，分别训练多个树模型，最终将结果整合到一起即可，最具代表性的是随机森林
### 随机森林

通过样本随机采样+特征随机采样获得多个不同的决策树，然后综合各个决策树的结果来获取最终的结果,在分类问题中众数就是最终的分类结果，回归问题中求平均值即可。实际问题中树模型的个数一般取100到200个。

集成算法中有个很实用的参数特征重要性。也就是树模型中哪些特征被利用的更多，因为树模型会优先选择最有价值的特征。综合考虑所有的树模型，如果有一个特征在大部分树模型中都被利用且靠近根节点，它就比较重要。bagging集成相当于是半自动进行特征选择，这在特征工程中一定程度上省时省力，适用于较高维度的数据，而且可以进行特征重要性评估。

当使用树模型时会清晰地得到整个分裂过程，方便进行可视化分析，这是其他算法望尘莫及的。

### Boosting算法

如果说bagging是并行的训练一群基础模型，boosting算法则是串行的训练，每一个基础模型都会对后面模型的训练有影响。

公式:

#### Adaboost算法



###  

### 实战

1.数据预处理，特征展示，完成建模，可视化展示

2.分析数据样本量是特征个数对结果的影响，在保证算法一直的前提下增加数据样本个数，观察结果变化，重新考虑特征工程，引入新特征后观察结果走势。

3.进行调参，找到最合适的参数，掌握机器学习中两种经典调参方法，对当前模型选择最合适的参数。



#### 看看数据规模

features.shape

#### 进一步查看统计特征

features.describe()

看count发现没有缺失值

#### 预处理

将时间转化为标准时间(略过



更直观地观察数据，导入Matpltlib包

选择风格

展示四项指标:最高气温的标签值，前天昨天朋友预测的气温最高值，4个图，选择2*2的规模



原始数据中的week列不是数值特征，而是表示星期几的字符串，计算机并不认识这些数据，需要转换一下

常用转换方式:

1.onehot encoding

2.pandas中的get_dummies函数

```python
labels=np.array(features["actual"])
features=features.drop("actual",axis=1)
feature_list=list(features.columns)
features=np.array(features)
```



#### 训练模型前需要先对数据集进行划分

```python
from sklearn.model_selection import train_test_split
train_features,test_features,train_labels,test_labels=train_test_split(features,labels,test_size=0.25,random_state=42)
```



#### 开始训练:

先建立1000颗树模型试试，其他参数暂时用默认值，再深入调参任务

```python
from sklearn.ensemble import RandomForstRegressor
rf=RandomForstRegressor(n_estimators=1000,random_state=42)
rf.fit(train_features,train_labels)
```

选用MAPE指标进行评估，平均绝对百分误差

```python
predictions=rf.predict(test_features)
errors=abs(predictions - test_labels)
mape=100*(errors/test_labels)
print("MAPE:",np.mean(mape))
```

#### 树模型的可视化

##### 安装Graphviz工具

1.安装

2.配置环境变量

3.验证安装

4.pip中安装graphviz,pydot2,pydotplus,pydot

##### 绘制决策树模型

```python
from sklearn.tree import export_graphviz
import pydot
tree=rf.estimators_[5]
//导出成.dot文件
export_graphviz(tree.oyt_file="tree.dot",feature_names=feature_list,rounded= True,precision=1)
//绘图
(graph,)=pydot.graph_from_dot_file("tree.dot")
graph.write_png("tree.png")
```

标识中非叶子结点包含了四项指标:所选特征与切分点，评估结果，此节点样本数量，节点预测结果?



#### 特征重要性

调用sklearn工具包中现有的函数

```python
importtances=list(rf.feature_importances_)
#转换格式
feature_importances=[(feature,round(importance,2))for feature,importance in zip(feature_list,importances)]
#排序
feature_importances=sorted(feature_importances,key=lambda x:x[1],reverse=True)
#对应进行打印
[print("Variable:{:20}Importance:{}".format(*pair))for pair in feature_importances]
```

上述输出结果分别打印了当前特征及其对应的特征重要性，绘制成图标分析起来更加容易

```python
#转换为List格式
x_values=list(range(len(importances)))
#绘图
plt.bar(x_values,importances,orientation="vertical")
plt.xticks(x_values,feature_list,rotation="vertical")
plt.ylabel("Importance")
plt.xlabel("Variable")
plt.title("Variable Importances")
```



最后看看预测的效果
```python
true_data=pd.DataFrame(data={"date":dates,"actual":labels})
predictions_data=pd.DataFrame(data={"date":test_dates,"predication":predictions})
plt.plot(true_data["date"],true_data["actual"],"b-",label="actual")
plt.plot(predictions_data["date"],predictions_data["prediction"],"rp",label="prediction")
plt.xticks(roations="60")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Maximum Temperature(F)")
plt.title("Actual and Predicted Values")
```

通过上述输出结果的走势可以看出模型已经基本可以掌握天气变化情况，接下来要深入数据

```python
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV


def forest():
    # 加载数据
    titan = pd.read_csv("E:/Desktop/机器学习_新/数据集/泰坦尼克数据集/train.csv")

    # 构造特征值和目标值
    feature = titan[["Pclass", "Age", "Fare", "Sex"]]
    target = titan["Survived"]

    # 特征预处理
    # 查看有没有缺失值
    print(pd.isnull(feature).any())
    # 填充缺失值
    Age = feature.pop("Age")  # 取出，意思是取出来之后删除原来的
    Age = Age.fillna(Age.mean())
    feature.insert(0, "Age", Age)

    # 字典特征抽取
    dv = DictVectorizer()
    feature = dv.fit_transform(feature.to_dict(orient="records"))
    feature = feature.toarray()
    print(feature)
    print(dv.get_feature_names())

    # 划分数据集
    x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.25)
    print("训练集：", x_train.shape, y_train.shape)
    print("测试集：", x_test.shape, y_test.shape)

    # 建立模型
    rf = RandomForestClassifier()

    # 超参数搜索
    param = {"n_estimators":[10, 20, 30, 40], "max_depth":[25, 35, 45]}
    gc = GridSearchCV(rf, param_grid=param, cv=5)

    # 训练
    gc.fit(x_train, y_train)

    # 交叉验证网格搜索的结果
    print("在测试集上的准确率：", gc.score(x_test, y_test))
    print("在验证集上的准确率：", gc.best_score_)
    print("最好的模型参数：", gc.best_params_)
    print("最好的模型：", gc.best_estimator_)


if __name__ == "__main__":
    forest()
```

