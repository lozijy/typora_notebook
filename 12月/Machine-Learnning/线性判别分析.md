# 线性判别分析(LDA)

## 用投影进行分类

通过线性投影最小化同类样本的差异，最大化不同类样本差异

投影矩阵W,特征向量x投影后的新向量

![image-20221128192315247](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192315247.png)

![image-20221128192359849](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192359849.png)

图中的特征向量本来是二维的，通过投影转换成了一维，并且完全分开了，因此LDA既可以用来分类，也可以用来降维.

## 投影矩阵

### 一维的情况

关键是找到最佳投影矩阵，有一个向量w,所有向量对这个向量做投影可以得到一个标量

![image-20221128192811259](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192811259.png)

#### 类间距最大 

用均值进行比较

投影前每类的均值

![image-20221128193240004](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193240004.png)

投影后每类的均值

![image-20221128193313559](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193313559.png)

二者作差

![image-20221128193331096](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193331096.png)

#### 类内距最小

类内的差异大小用方差衡量，定义*类内散布*如下:

![image-20221128193215294](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193215294.png)



优化函数:

![image-20221128193454932](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193454932.png)

通过让这个这个优化函数最大我们可以算出向量w,为了写出关于w的函数，我们定义*类内散布矩阵*

![image-20221128193708125](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193708125.png)

总类内散布矩阵:

![image-20221128193737157](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193737157.png)

类内散布矩阵和类内散布的关系:

![image-20221128193929183](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193929183.png)

于是总类内散布矩阵和总类内散布的关系:

![image-20221128194050698](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194050698.png)





上面我们改写了分母，下面改写分子类间距:

![image-20221128194331148](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194331148.png)

我们把中间的![image-20221128194401304](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194401304.png)

定义为SB



则整个优化函数变成了:

![image-20221128194437072](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194437072.png)

这里我们自行加上一个约束条件,原因是可以证明如果w是最优解，则我们加上一个系数k,则w'还是最优解

![image-20221128194533553](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194533553.png)

则最优化问题转化成了带约束条件的最优化问题，我们使用拉格朗日法解决:

构造拉格朗日乘子函数:

![image-20221128194737104](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194737104.png)

对w求梯度并使梯度为0得到

![image-20221128194831018](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194831018.png)

如果Sw可逆，则左右同时左乘一个Sw的逆，得到

![image-20221128195005118](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195005118.png)

这相当于是一个特征方程,所以λ左边那个矩阵的特征值,w是对应的特征向量，

λ和w是![image-20221128195320063](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195320063.png)的解

最后带入优化函数得到:

![image-20221128195417553](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195417553.png)

所以最大的特征值λ和对应的特征向量w就是最优解

![image-20221128214849817](https://raw.githubusercontent.com/lozijy/image/main/image-20221128214849817.png)

#### 一维的分类

如何分类?比较它与其他所有类的均值的距离，取最小的那个为分类的结果

![image-20221128195712546](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195712546.png)

### 推广到高维

类内散布矩阵

![image-20221128231852204](https://raw.githubusercontent.com/lozijy/image/main/image-20221128231852204.png)

单个类的类内散布矩阵

![image-20221128231944003](https://raw.githubusercontent.com/lozijy/image/main/image-20221128231944003.png)

mi是单个类的均值向量

引入:总体均值向量

![image-20221128232042812](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232042812.png)

引入:总体散布矩阵

![image-20221128232113620](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232113620.png)

总体散布矩阵和类内散布矩阵,类间散布矩阵的关系:

![image-20221128232212710](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232212710.png)

推导如下:

![image-20221128232316832](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232316832.png)

从d维向c-1维投影变成矩阵和向量的乘积:

![image-20221128232557121](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232557121.png)

W是d*(c-1)维的矩阵，可以证明，最后的目标为求解下面的最优化问题:

![image-20221128232647789](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232647789.png)

tr为矩阵的迹,同样构造拉格朗日函数可以证明使该目标函数最大的W的列w满足

![image-20221128232817687](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232817687.png)

最优解还是![image-20221128232845242](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232845242.png)

的特征值和特征向量

## 主成分分析和线性判别分析的区别

虽然最后都归结为求解矩阵的特征值问题，前者是无监督的机器学习方法，后者要计算类内和类间散度矩阵使用了样本标签值，是有监督的机器学习方法

前者是最小化重构误差，后者是最大化类间差异和最小化类内差异

## 实战

iris数据集

![image-20221128215907031](https://raw.githubusercontent.com/lozijy/image/main/image-20221128215907031.png)

 

加载数据集

```python
from os import startfile
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

def load_iris(file_path='./data/iris.csv',test_size=0.3):
    """
    功能：加载数据集并划分训练集和测试集
    """
    df = pd.read_csv(file_path)
    x,y = df.iloc[:,0:-1].values,df.iloc[:,-1].values
    #打乱并划分测试集和训练集
    train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=test_size,random_state=5)
    return train_x,test_x,train_y,test_y
```

LDA类

```python
import numpy as np

class LDA(object):
    """
    线性判别分析
    """
    def __init__(self,data,target,d) -> None:
        self.data = data
        self.target = target
        self.d = d
        self.labels = set(target)
        self.mu = self.data.mean(axis=0)
    
    def divide(self):
        """
        功能：将传入的数据集按target分成不同的类别集合并求出对应集合的均值向量
        """
        self.classify,self.classmu = {},{}
        for label in self.labels:
            self.classify[label] = self.data[self.target==label]
            self.classmu[label] = self.classify[label].mean(axis=0)
    
    def getSt(self):
        """
        功能：计算全局散度矩阵
        """
        self.St = np.dot((self.data-self.mu).T,(self.data-self.mu))

    def getSb(self):
        """
        功能：计算类内散度矩阵
        """
        self.Sb = np.zeros((self.data.shape[1],self.data.shape[1]))
        for i in self.labels:
            #获取类别i样例的集合
            classi = self.classify[i]
            #获取类别i的均值向量
            mui = self.classmu[i]
            self.Sb += len(classi) * np.dot((mui - self.mu).reshape(-1,1),(mui - self.mu).reshape(1,-1))

    def getW(self):
        """
        功能：计算w
        """
        self.divide()
        self.getSt()
        self.getSb()
        #St = Sw + Sb
        self.Sw = self.St - self.Sb 
        #计算Sw-1*Sb的特征值和特征向量
        #eig_vectors[:i]与 eig_values相对应
        eig_values, eig_vectors = np.linalg.eig(np.linalg.inv(self.Sw).dot(self.Sb))
        #寻找d个最大非零广义特征值
        topd = (np.argsort(eig_values)[::-1])[:self.d]
        #用d个最大非零广义特征值组成的向量组成w
        self.w = eig_vectors[:,topd]
        



if __name__ == "__main__":
    x = np.array([[1,2,3],[2,1,3],[2,4,1],[1,3,2],[3,6,4],[3,1,1]])
    y = np.array([0,1,2,0,1,2])
    lda = LDA(x,y,2)
    lda.getW()

```



```python
from LDAModel import LDA
from load_data import load_iris
import numpy as np
from scipy.stats import norm
from sklearn.metrics import accuracy_score

def judge(gauss_dist,x):
    """
    功能：判断样本x属于哪个类别
    """
    #将样本带入各个类别的高斯分布概率密度函数进行计算
    outcome = [[k,norm.pdf(x,loc=v['loc'],scale=v['scale'])] for k,v in gauss_dist.items()]
    #寻找计算结果最大的类别
    outcome.sort(key=lambda s:s[1],reverse=True)
    return outcome[0][0]

def Test():
    """
    功能：对测试集进行分类并返回准确率
    """
    #加载数据集
    train_x,test_x,train_y,test_y = load_iris()
    #创建模型
    lda = LDA(train_x,train_y,1)
    #获取投影矩阵w
    lda.getW()
    #对训练集进行降维
    train_x_new = np.dot((train_x), lda.w)
    #获取训练集各个类别对应的高斯分布的均值和方差
    gauss_dist = {}
    for i in lda.labels:
        classi = train_x_new[train_y==i]
        loc = classi.mean()
        scale = classi.std()
        gauss_dist[i] = {'loc':loc,'scale':scale}
    test_x_new = np.dot(test_x,lda.w)
    pred_y = np.array([judge(gauss_dist,x) for x in test_x_new])
    
    return accuracy_score(test_y,pred_y)

if __name__ == "__main__":
    acc = Test()
    print(acc)
```

