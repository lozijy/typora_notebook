# 线性判别分析(LDA)

## 用投影进行分类

通过线性投影最小化同类样本的差异，最大化不同类样本差异

投影矩阵W,特征向量x投影后的新向量

![image-20221128192315247](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192315247.png)

![image-20221128192359849](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192359849.png)

图中的特征向量本来是二维的，通过投影转换成了一维，并且完全分开了，因此LDA既可以用来分类，也可以用来降维.

## 投影矩阵

### 一维的情况

关键是找到最佳投影矩阵，有一个向量w,所有向量对这个向量做投影可以得到一个标量

![image-20221128192811259](https://raw.githubusercontent.com/lozijy/image/main/image-20221128192811259.png)

#### 类间距最大 

用均值进行比较

投影前每类的均值

![image-20221128193240004](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193240004.png)

投影后每类的均值

![image-20221128193313559](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193313559.png)

二者作差

![image-20221128193331096](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193331096.png)

#### 类内距最小

类内的差异大小用方差衡量，定义*类内散布*如下:

![image-20221128193215294](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193215294.png)



优化函数:

![image-20221128193454932](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193454932.png)

通过让这个这个优化函数最大我们可以算出向量w,为了写出关于w的函数，我们定义*类内散布矩阵*

![image-20221128193708125](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193708125.png)

总类内散布矩阵:

![image-20221128193737157](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193737157.png)

类内散布矩阵和类内散布的关系:

![image-20221128193929183](https://raw.githubusercontent.com/lozijy/image/main/image-20221128193929183.png)

于是总类内散布矩阵和总类内散布的关系:

![image-20221128194050698](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194050698.png)





上面我们改写了分母，下面改写分子类间距:

![image-20221128194331148](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194331148.png)

我们把中间的![image-20221128194401304](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194401304.png)

定义为SB



则整个优化函数变成了:

![image-20221128194437072](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194437072.png)

这里我们自行加上一个约束条件,原因是可以证明如果w是最优解，则我们加上一个系数k,则w'还是最优解

![image-20221128194533553](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194533553.png)

则最优化问题转化成了带约束条件的最优化问题，我们使用拉格朗日法解决:

构造拉格朗日乘子函数:

![image-20221128194737104](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194737104.png)

对w求梯度并使梯度为0得到

![image-20221128194831018](https://raw.githubusercontent.com/lozijy/image/main/image-20221128194831018.png)

如果Sw可逆，则左右同时左乘一个Sw的逆，得到

![image-20221128195005118](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195005118.png)

这相当于是一个特征方程,所以λ左边那个矩阵的特征值,w是对应的特征向量，

λ和w是![image-20221128195320063](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195320063.png)的解

最后带入优化函数得到:

![image-20221128195417553](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195417553.png)

所以最大的特征值λ和对应的特征向量w就是最优解

![image-20221128214849817](https://raw.githubusercontent.com/lozijy/image/main/image-20221128214849817.png)

#### 一维的分类

如何分类?比较它与其他所有类的均值的距离，取最小的那个为分类的结果

![image-20221128195712546](https://raw.githubusercontent.com/lozijy/image/main/image-20221128195712546.png)

### 推广到高维

类内散布矩阵

![image-20221128231852204](https://raw.githubusercontent.com/lozijy/image/main/image-20221128231852204.png)

单个类的类内散布矩阵

![image-20221128231944003](https://raw.githubusercontent.com/lozijy/image/main/image-20221128231944003.png)

mi是单个类的均值向量

引入:总体均值向量

![image-20221128232042812](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232042812.png)

引入:总体散布矩阵

![image-20221128232113620](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232113620.png)

总体散布矩阵和类内散布矩阵,类间散布矩阵的关系:

![image-20221128232212710](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232212710.png)

推导如下:

![image-20221128232316832](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232316832.png)

从d维向c-1维投影变成矩阵和向量的乘积:

![image-20221128232557121](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232557121.png)

W是d*(c-1)维的矩阵，可以证明，最后的目标为求解下面的最优化问题:

![image-20221128232647789](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232647789.png)

tr为矩阵的迹,同样构造拉格朗日函数可以证明使该目标函数最大的W的列w满足

![image-20221128232817687](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232817687.png)

最优解还是![image-20221128232845242](https://raw.githubusercontent.com/lozijy/image/main/image-20221128232845242.png)

的特征值和特征向量

## 主成分分析和线性判别分析的区别

虽然最后都归结为求解矩阵的特征值问题，前者是无监督的机器学习方法，后者要计算类内和类间散度矩阵使用了样本标签值，是有监督的机器学习方法

前者是最小化重构误差，后者是最大化类间差异和最小化类内差异

## 实战



![image-20221128215907031](https://raw.githubusercontent.com/lozijy/image/main/image-20221128215907031.png)

 