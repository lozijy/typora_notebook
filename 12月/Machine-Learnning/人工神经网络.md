# 人工神经网络

## sigmoid激活函数

![image-20221130155202837](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155202837.png)

图像:

![image-20221130155226232](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155226232.png)

单增,导数是:![image-20221130155257401](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155257401.png)

当x是0的时候导数最大.

## 神经元

输入向量和权重向量相乘后加上偏置项是一个标量，代入激活函数中后仍然是一个标量

![image-20221130154943049](https://raw.githubusercontent.com/lozijy/image/main/image-20221130154943049.png)

神经元的输出值:一个标量

![image-20221130154933983](https://raw.githubusercontent.com/lozijy/image/main/image-20221130154933983.png)

## 神经网络

### 权重矩阵

这一层的多个神经元数个标量组合又成了一个输入向量，这样循环往复就是我们的神经网络

![image-20221130155122750](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155122750.png)

每一层每一个神经元对应一个权重，是一个向量，如果我们把这一层的所有神经元的权重向量组合在一起就是一个权重矩阵,每一行表示一个神经元对应的权重向量

我们以这样的形式表示某一层的权重矩阵:



### 输出

第二层的输出值:

![](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155849947.png)

第三层的输出值:

![image-20221130155917932](https://raw.githubusercontent.com/lozijy/image/main/image-20221130155917932.png)

### 为什具有良好的拟合性

神经网络通过激活函数而具有非线性，而通过调整权重形成不同的映射函数



## 正向传播算法

假设神经网络的输入是n维向量x，输出是m维向量y,那么权重矩阵是m*n大小的，实现了如下向量的映射![image-20221130160314034](https://raw.githubusercontent.com/lozijy/image/main/image-20221130160314034.png)

用于分类问题时，比较输出向量每个分量的大小，求最大值，最大值对应的标签就是分类的结果，对于回归问题，直接将输出向量作为回归值

算法流程:

![image-20221130160552974](https://raw.githubusercontent.com/lozijy/image/main/image-20221130160552974.png)

## 反向传播算法

对于m个样本![image-20221130160715167](https://raw.githubusercontent.com/lozijy/image/main/image-20221130160715167.png),我们用欧式距离作为优化目标:

![image-20221130160658821](https://raw.githubusercontent.com/lozijy/image/main/image-20221130160658821.png)



优化函数的自变量是每层的权重矩阵Wi和偏执向量bi，无法保证目标函数是凸函数，有陷入局部最小的风险。所以我们使用梯度下降法，最重要的是算出优化函数对权重矩阵和偏执向量的梯度，首先考虑对单个样本的损失函数

### 对于单个样本

优化函数:

![image-20221130161136860](https://raw.githubusercontent.com/lozijy/image/main/image-20221130161136860.png)

![image-20221130161204480](https://raw.githubusercontent.com/lozijy/image/main/image-20221130161204480.png)

考虑i=1,则分子后面那一坨是常数，扔掉，i=2时扔掉前面那一坨，然后用链式求导法则分别算出

i=1时

![image-20221130165408329](https://raw.githubusercontent.com/lozijy/image/main/image-20221130165408329.png)

i=2时

![image-20221130165429968](https://raw.githubusercontent.com/lozijy/image/main/image-20221130165429968.png)

综合一下:

![image-20221130165502484](https://raw.githubusercontent.com/lozijy/image/main/image-20221130165502484.png)

第一个下标i决定了权重矩阵第i行和偏执向量的第i个分量，这可以看成是一个列向量和一个行向量相乘的结果，写成矩阵形式是:

![image-20221130165944220](https://raw.githubusercontent.com/lozijy/image/main/image-20221130165944220.png)

### 扩展到多个样本

## 理论解释

## 面临的问题

1梯度消失

反向传播算法计算误差项时每一层都要乘以本层激活函数的导数

![image-20221130170825972](https://raw.githubusercontent.com/lozijy/image/main/image-20221130170825972.png)

如果激活函数的导数小于1，多次连成后梯度很快会接近于0。

反之如果激活函数的导数大于1，多次连乘后梯度会趋于非常大的值



2.退化

神经网络的训练误差和测试误差会随着层数的增大而增大.



3.局部极小值

神经网络的损失函数一般不是凸函数，有可能会陷入局部极小值的情况

4.鞍点

鞍点指梯度为0但Hessian矩阵不定的情况，这不是局部极值点

## 实现的细节

### 输入值和输出值

输入变量归一化

输出变量一般使用编码向量的方法，将输出向量设置为k维，如果样本属于第k类，则将输出向量的第k个分量设置为1,其他设置为0,预测时计算输出向量分量的最大值，这个值的分量号就是分类结果，这种方式叫One-hot编码

### 网络规模

近几年随着深度学习的兴起网络的规模不断扩大，目前最大的网络层数已经超过1000层，神经元个数到达百亿级，接近人脑的神经元个数

### 激活函数

其他常见激活函数:对称型sigmoid函数,tahn(双曲正切函数)，幂函数，ReLU(修正线性单元)等，人们要求激活函数处处可导

![image-20221130172113254](https://raw.githubusercontent.com/lozijy/image/main/image-20221130172113254.png)

![image-20221130172124253](https://raw.githubusercontent.com/lozijy/image/main/image-20221130172124253.png)

### 损失函数

其他损失函数:交叉熵，对比损失函数，分类问题下交叉熵一般比欧式距离有更好的表现。

### 权重初始化

作为梯度下降法的初始值，不能简单的初始化为0或1,一般使用服从某种分布的随机数

### 正则化

防止出现过拟合问题,

方式

1.在损失函数中加上正则化项

2.dropout

L2正则化:

![image-20221130172945916](https://raw.githubusercontent.com/lozijy/image/main/image-20221130172945916.png)

L1正则化:![image-20221130172933857](https://raw.githubusercontent.com/lozijy/image/main/image-20221130172933857.png)