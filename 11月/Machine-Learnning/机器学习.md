## K近邻

##　数据降维

### 主成分分析

### 流形学习！



## 线性判别分析

前面学的主成分分析主要思想是把高维特征映射到低维空间中去，但是对于无监督学习来说效果并不好,所以我们新研发了一种线性判别分析

### 一维的情况

思想是最大化类间差距，最小化类间差距.

用y=WX将x投影到y上

考虑一维的情况

   表示i种数据集平均值

   是所有数据的平均值

​    是投影后的平均值  

​    表示投影后的均方误差

那么我们主要让这个式子极大化 ： 

定义类内散布矩阵:

定义散布矩阵:

这样分母就变成了:

分子就变成了:

目标函数就转化成了:

*加上一个约束条件消除冗余:*

拉格朗日:

求梯度让梯度为零:

这是一个广义的特征方程，如果可逆，则有:

假设  和  是特征方程的解，带入目标函数则得到:

目标是让这个比值最大化，因此，最大的特征值和对应的特征方程就是解。

### 推广到高维

思想是最大化类间差距，最小化类间差距.

用y=WX将x投影到y上

考虑一维的情况

   表示i种数据集平均值

   是所有数据的平均值

​    是投影后的平均值  

​    表示投影后的均方误差

那么我们主要让这个式子极大化 ： 

定义类内散布矩阵:

定义散布矩阵:

这样分母就变成了:

分子就变成了:

目标函数就转化成了:

*加上一个约束条件消除冗余:*

拉格朗日:

求梯度让梯度为零:

这是一个广义的特征方程.解法不太一样，是对    进行特征值分解(奇异值分解)