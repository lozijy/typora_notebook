KNN最邻近分类算法是数据挖掘分类中最简单的分类算法之一

使用你的邻居来推断出你的类别

KNN算法的关键:

1.所有特征都要做可比较的量化，如果样本特征中是非数值类型，需要将其量化为数值，比如颜色，转换成灰度值

2.样本特征需要做归一化处理，因为样本中的不同参数的定义域和取值范围对距离计算的影响不一样，如果不进行归一化处理，则取值较大的参数的影响力会超过取值较小的参数

3.定义一个距离函数来计算两个样本之间的距离，常用的距离函数包括:欧式距离，余弦距离，汉明距离，曼哈顿距离等，一般用欧式距离作为距离度量，只能处理连续变量，而在文本分类这种非连续变量的情况下，汉明距离可以用来度量

![image-20221121190622263](C:\Users\lonux\AppData\Roaming\Typora\typora-user-images\image-20221121190622263.png)

4.确定K的值，K太大容易引起欠拟合，K太小容易引起过拟合

KNN的优点:

简单，易理解，无需训练，无需估计参数

适合对稀有事件进行分类，特别适合多分类问题(对象有多个标签)，此时KNN的效果比SVM表现的更好

KNN的缺点:

计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。

可理解性差，无法给出像决策树那样的规则。



